---
title: "MICR Multiverse Analysis"
output: html_notebook
---

# Set up
## Packages
```{r, warning=F}
# Wrangling
library(tidyverse)
library(mgsub)

# Statistics/Numerical processing
library(lme4)
library(brms)
library(boot)

# Plotting
library(ggplot2)
library(ghibli)

# Optional settings
options(dplyr.summarise.inform=F) # Stop dplyr from printing summarise error (that isn't an error)
select <- dplyr::select # Ensure that select() command is the dplyr command (clashes with MASS, which is imported/required by paran)
```

## Load Data
```{r}
load(file="./data/au_data_coded.rData")
load(file="./data/xints_fixef_au.rData")
load(file="./data/xints_coef_au.rData")
```


## Preparation & Testing
### Function for Outlier Options
```{r}
# For each value of quantile, calculate number of slope outliers per type of analysis

get_ind_df <- function (df, i) {
  slope_lower = quantile(df$slope_pred, i)
  Q1 <- quantile(df$xint_pred, .25)
  Q3 <- quantile(df$xint_pred, .75)
  IQR <- IQR(df$xint_pred)

  au_data_ind_max <- au_data_coded %>% select(-step:-rt) %>%
    merge(., df) %>%
    mutate(guiseShift = ifelse(speakerGuise == 'CN', -xint_pred, xint_pred)) %>%
    mutate(slope_out = ifelse(slope_pred < slope_lower & slope_pred > -slope_lower, TRUE, FALSE)) %>%
    # mutate(xint_out = ifelse(xint_pred > Q1-IQR*1.5 & xint_pred < Q3+IQR*1.5, FALSE, TRUE)) %>%
    distinct() %>% droplevels()
  
  au_data_ind <- au_data_ind_max %>% filter(slope_out != TRUE) %>% filter(speakerGuise!='BL')
}
```

```{r}
outlier_sensitivity <- function (df) {
  outlier_sens <- data.frame(quantile_val=as.integer(NA),
                        n_outliers=as.integer(NA),
                        max_slope=as.integer(NA),
                        stringsAsFactors=FALSE)
  
  for (i in seq(0,1,0.01)) {
    
    slope_lower = quantile(df$slope_pred, i)
    Q1 <- quantile(df$xint_pred, .25)
    Q3 <- quantile(df$xint_pred, .75)
    IQR <- IQR(df$xint_pred)
  
    au_data_ind_max <- au_data_coded %>% select(-step:-rt) %>%
      merge(., df) %>%
      mutate(guiseShift = ifelse(speakerGuise == 'CN', -xint_pred, xint_pred)) %>%
      mutate(slope_out = ifelse(slope_pred < slope_lower & slope_pred > -slope_lower, TRUE, FALSE)) %>%
      mutate(xint_out = ifelse(xint_pred > Q1-IQR*1.5 & xint_pred < Q3+IQR*1.5, FALSE, TRUE)) %>%
      filter(speakerGuise!='BL') %>%
      distinct() %>% droplevels() 
    
    au_data_ind <- au_data_ind_max %>% filter(slope_out != TRUE)
    
    outs <- setdiff(au_data_ind_max, au_data_ind)
    quantile_val = i
    n_outliers = nrow(outs)
    max_slope = max(outs$slope_pred)
    
    row <- cbind.data.frame(quantile_val, n_outliers,  max_slope)
    outlier_sens <- rbind(outlier_sens, row) %>% na.omit()
  }
  return(outlier_sens)
}
```

### Collect estimates
```{r}
( outlier_sens_coef <- outlier_sensitivity(xints_coef_au) )
```

```{r}
( outlier_sens_fixef <- outlier_sensitivity(xints_fixef_au) )
```

### Visualize
```{r}
outlier_sens_coef %>%
  ggplot() +
  scale_x_continuous(breaks=seq(0, 1, 0.05)) +
  scale_y_continuous(breaks=seq(0, 2, 0.1)) +
  theme_bw() +
  geom_line(aes(x=quantile_val, y=max_slope)) +
  geom_line(aes(x=quantile_val, y=max_slope), col='red', data=outlier_sens_fixef)

outlier_sens_coef %>%
  ggplot() +
  scale_x_continuous(breaks=seq(0, 1, 0.05)) +
  scale_y_continuous(breaks=seq(0, 240, 10)) +
  theme_bw() +
  geom_line(aes(x=quantile_val, y=n_outliers)) +
  geom_line(aes(x=quantile_val, y=n_outliers), col='red', data=outlier_sens_fixef)

```

### Models
```{r}
ind_au_coef <- get_ind_df(xints_coef_au, 0.06)
ind_au_lm <- lm(guiseShift ~ scale(Age) + genderContr + CEscore_all + 
                  guiseContr * SAscore_all * EQscore_all, 
                  data=ind_au_coef)
# plot(ind_au_lm)
summary(ind_au_lm)
```


```{r}
ind_au_fixef <- get_ind_df(xints_fixef_au, 0.06)
ind_au_lm <- lm(guiseShift ~ scale(Age) + genderContr + CEscore_all + 
                  guiseContr * SAscore_all * EQscore_all, 
                  data=ind_au_fixef)
# plot(ind_au_lm)
summary(ind_au_lm)
```

#### Extract p-values
```{r}
pval <- data.frame(summary(ind_au_lm)$coefficients[,4]) %>% 
  rownames_to_column("predictor") %>% 
  rename(pvalue = 2)
pval
```

#### Check Residuals
No relation apparent.
##### Coef
```{r}
ind_au_resid <- data.frame(resid(ind_au_lm)) %>% rename(resid=resid.ind_au_lm.)

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_coef$SAscore_all)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_coef$EQscore_all)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_coef$CEscore_all)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_coef$genderContr)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_coef$guiseContr)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=scale(ind_au_coef$Age))) +
  geom_point() +
  geom_smooth(method='lm')
```
##### Fixef
```{r}
ind_au_resid <- data.frame(resid(ind_au_lm)) %>% rename(resid=resid.ind_au_lm.)

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_fixef$SAscore_all)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_fixef$EQscore_all)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_fixef$CEscore_all)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_fixef$genderContr)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=ind_au_fixef$guiseContr)) +
  geom_point() +
  geom_smooth(method='lm')

ind_au_resid %>%
  ggplot(aes(y=resid, x=scale(ind_au_fixef$Age))) +
  geom_point() +
  geom_smooth(method='lm')
```


#### Version: Select
```{r}
ind_au_coef <- get_ind_df(xints_coef_au, 0.06)
ind_au_lm <- lm(guiseShift ~ scale(Age) + CEscore_all + genderContr + 
                  guiseContr * SAscore_all * EQscore_all, 
                  data=ind_au_coef)
summary(ind_au_lm)
```

#### Version: All
```{r}
ind_au_coef <- get_ind_df(xints_coef_au, 0.06)
ind_au_lm <- lm(guiseShift ~ scale(Age) + CEscore_all * genderContr * guiseContr * 
                  SAscore_all * EQscore_all, 
                  data=ind_au_coef)
summary(ind_au_lm)
```


```{r}
ind_au_fixef <- get_ind_df(xints_fixef_au, 0.06)
ind_au_lm <- lm(guiseShift ~ scale(Age) + CEscore_all * genderContr * guiseContr * 
                  SAscore_all * EQscore_all, 
                  data=ind_au_fixef)
summary(ind_au_lm)
```





# Multiverse / Sensitivity Analysis
Following method in Steegan et al. (2016).

## List Reasonable Choices

1. Score calculation [not testing (yet)]
      (a) PCA composite score of the questions
      (b) Average score of the questions
  
2. Crossover Point Extraction
      (a) Coefficients from a no-Guise model based on random effects
      (b) Fixed effect estimates from individual models per participant

3. Outlier Removal
      (a) No exclusion
      (b) ... 0.01 - 0.30 quantile based on slope only

Excluding from individual analysis participants who have a *flat slope* across step is reasonable because it means that they did not distinguish the two end point tokens as being raised/unraised. In this case, there is no observed perceptual function to be shifted (which is the effect being tested), so any variation in crossover point appears to be just noise. In other words, any value of the crossover point (x-intercept) for such an individual with a *flat slope* is a meaningless measure since it is supposed to measure the _single point_ or _value of step_ at which participants are perceiving raised tokens 50% of the time, but these participants apparently perceived the _whole continuum_ as 50% raised (did not distinguish raising by step).

The question here is what value of slope is "too low" to render the crossover point meaningless? What value is "too close to zero"? Since the values range continuously from values close to 0 to 1.XX, there is no natural breaking point. The choice appears to be arbitrary, even when trying to balance the number of individuals who would be removed from analysis based on a certain cut-off point (of which "too many" is also arbitraryâ€”5% of the data? 10%? 15%?). 

4. Model Specification
      (a) Key interactions only 
      (b) Additional CEscore interaction 
      (c) Additional Gender interaction 
      (d) Additional CEscore + Gender interaction 

The corresponding model specification would be:
(a) guiseShift ~ scale(Age) + genderContr + CEscore_all + guiseContr * SAscore_all * EQscore_all;
(b) guiseShift ~ scale(Age) + genderContr + CEscore_all * guiseContr * SAscore_all * EQscore_all;
(c) guiseShift ~ scale(Age) + CEscore_all + genderContr * guiseContr * SAscore_all * EQscore_all;
(d) guiseShift ~ scale(Age) + CEscore_all * genderContr *  guiseContr * SAscore_all * EQscore_all

## Run Dataset Multiverse

For each combination of reasonable options:
1. Prepare the data (e.g., extract x-intercepts with one method, apply one outlier cut-off)
2. Run the Key Interactions model
3. Extract the p-value and save in dataframe

With these values:
4. Plot a histogram of each p-value including a line at a=0.05.
5. Plot a grid of each combination of options, coloring in the cell if value is significant.

### X-int Method + Slope Outlier

For this first test analysis, I will vary the X-intercept extraction method (coef vs. fixef) and the slope outlier quantile value (from 0 to 0.30 in steps of 0.01 quantiles). The number of combinations is 2 * 31 = 62. I will try out the analysis looking at p-values for all predictors, using the other predictors as comparisons to the predictors of interest (particularly the SA x EQ interaction).

#### MV Function
```{r}
multiverse_pvals <- function (df, xint_type, max_quantile) {
  out_df <- data.frame(xint_method=as.factor(NA),
                        slope_quantile=as.integer(NA),
                        predictor=as.factor(NA),
                        pvalue=as.integer(NA),
                        stringsAsFactors=FALSE)
  
  for (i in seq(0, max_quantile, 0.05)) {
    
    ind_au_df <- get_ind_df(df, i)
    ind_au_lm <- lm(guiseShift ~ scale(Age) + genderContr + CEscore_all + 
                  guiseContr * SAscore_all * EQscore_all, 
                  data=ind_au_df)
    
    out_pval <- data.frame(summary(ind_au_lm)$coefficients[,4]) %>% 
      rownames_to_column("predictor") %>% 
      rename(pvalue = 2) %>%
      mutate(xint_method = xint_type,
             slope_quantile = i) %>%
      relocate(c(xint_method, slope_quantile))
    
    out_df <- rbind(out_df, out_pval) %>% na.omit()
  }
  return(out_df)
}
```

#### Collect p-Values
```{r}
pvals <- multiverse_pvals(xints_coef_au, 'coef', 0.3) %>%
  rbind(multiverse_pvals(xints_fixef_au, 'fixef', 0.3)) %>%
  mutate(significant = case_when(pvalue < 0.05 ~ 1,
                                 pvalue < 0.1 ~ 0.5,
                                 TRUE ~ 0))
pvals
```

#### Plot Histogram of p-values

Based on these plots that show the p-values from each combination of options together without differentiating by option, we can see that the two predictors that stand out as having particularly many instances of p-values below 0.05 are: `SAscore:EQscore` and `guise:SAscore` (to some extent). Other predictors with an intermediate number of instances below 0.05 are: `SAscore`, `Age`, and `Intercept`.

*My interpretation: the predicted SA:EQ unteraction appears to be a robust effect across all possible combinations of x-intercept extract method and slope outlier cut-off value.*

```{r}
pvals %>%
  ggplot() +
  geom_histogram(aes(x=pvalue), col='blue', fill='lightblue', bins=50) +
  facet_wrap(~predictor) +
  geom_vline(xintercept=0.05, col='red', linetype='dashed') +
  theme_bw()
```

#### Plot Grid of Options

In these plots, medium grey cells represent instances where the result was p<0.05 ("significant"), light grey cells represent p<0.1 ("marginal"), and white represents p>0.1 ("non-significant"). I show only relevant (SA, EQ, SA:EQ) or notable (guise:SA) predictors here.

As expected based on the above histogram, there is just no effect found for EQ, regardless of data analysis choices that were made. 

For the remaining three predictors, we see that at low slope cut-offs (below 0.6-0.8 quantile), there is generally no effect found, compared to some values above. 

*This suggests that there is some strong/relevant influence (skewing?) of the results due to participants with low slope values (i.e., have particularly or almost fully flat slopes). This was my hypothesis, that x-intercepts extracted from functions that are extremely flat would result in noisy/meaningless cross-over point values.*

In addition, we see a region of cut-off values where we see significant p-values for all three predictors, between ~0.6-0.14. However, above that, we see no significance (for SA) or spotty/inconsistent significance with marginal effects filling in some gaps (guise:SA and SA:EQ). However, the effect still seems more stable for SA:EQ than guise:SA across both x-int methods and cut-off scores. 

*My interpretation: These effects are quite sensitive to outlier removal across different values of slope quantile, which may indicate that particular individuals (likely with particularly large priming magnitude effect) are very influential. There is a reasonable explanation for why inclusion of more low-slope individuals leads to null results. For regions above, it may be that certain participants in that range have particularly influential x-intercept values (whether following or not following the general group trends), and this would be completely unrelated to their degree of slope (as long as they did differentiate across the continuum to some degree).*

```{r}
pvals %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all' |
                   predictor == 'EQscore_all') %>%
  ggplot(aes(x=xint_method, y=slope_quantile)) +
  scale_y_continuous(breaks=seq(0, 0.3, 0.05)) +
  facet_wrap(~predictor, ncol=4) +
  geom_tile(aes(fill=significant), linetype='solid', col='black') +
  geom_text(aes(label = round(pvalue, 3)), col='black') +
  scale_fill_gradient(low = "white", high = "grey") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

#### Summary Stats

If the mean of p-values are taken across all datsets (a possibility mentioned in Steegan et al., 2016), the values are large enough that they don't indicate reliable significance. 

If we take the median p-value, this measure of central tendency suggests that SA:EQ and marginally guise:SA may be considered robustly significant across research decisions, aligning with the insight provided by the histogram above.

```{r}
pvals %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all' |
                   predictor == 'EQscore_all') %>%
  group_by(predictor) %>%
  summarize(mean = mean(pvalue), median = median(pvalue), min=min(pvalue), max=max(pvalue))

pvals %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all' |
                   predictor == 'EQscore_all') %>%
  filter(xint_method=='coef') %>%
  group_by(predictor) %>%
  summarize(mean = mean(pvalue), median = median(pvalue), min=min(pvalue), max=max(pvalue))
```

## Run Model Multiverse

### X-int Method + Slope Outlier
This time, I will do the same thing but run each version of the model with more or fewer interactions.

That is, I will vary the X-intercept extraction method (coef vs. fixef) and the slope outlier quantile value (from 0 to 0.30 in steps of 0.01 quantiles), then run these datasets through four possible models: including (1) only key interactions (guise * SA * EQ), (2) add in CE but not Gender (guise * SA * EQ * CE), (3) add in Gender but not CE (guise * SA * EQ * Gender), and (4) add in both  (guise * SA * EQ * CE * Gender). 

The number of combinations is 2 * 31 * 4 = 248. I will try out the analysis looking at p-values for all of the predictors from the 'key interactions' model (because values would be skewed for predictors/interactions that only show up in the larger models), using the other predictors as comparisons to the predictors of interest (particularly the SA x EQ interaction).


#### MV Function
```{r}
model_multiverse_pvals <- function (df, xint_type, max_quantile) {
  out_df <- data.frame(xint_method=as.factor(NA),
                        slope_quantile=as.integer(NA),
                        model=as.factor(NA),
                        predictor=as.factor(NA),
                        pvalue=as.integer(NA),
                        stringsAsFactors=FALSE)
  
  for (i in seq(0, max_quantile, 0.05)) {
    
    ind_au_df <- get_ind_df(df, i)
    ind_au_lm <- lm(guiseShift ~ scale(Age) + genderContr + CEscore_all + 
                  guiseContr * SAscore_all * EQscore_all, 
                  data=ind_au_df)
    
    out_pval_1 <- data.frame(summary(ind_au_lm)$coefficients[,4]) %>% 
      rownames_to_column("predictor") %>% 
      rename(pvalue = 2) %>%
      mutate(xint_method = xint_type,
             slope_quantile = i,
             model = 'key_int') %>%
      relocate(c(xint_method, slope_quantile, model))
    
    ind_au_lm <- lm(guiseShift ~ scale(Age) + genderContr + CEscore_all * 
                  guiseContr * SAscore_all * EQscore_all, 
                  data=ind_au_df)
    
    out_pval_2 <- data.frame(summary(ind_au_lm)$coefficients[,4]) %>% 
      rownames_to_column("predictor") %>% 
      rename(pvalue = 2) %>%
      mutate(xint_method = xint_type,
             slope_quantile = i,
             model = 'ce_int') %>%
      relocate(c(xint_method, slope_quantile, model))
    
    ind_au_lm <- lm(guiseShift ~ scale(Age) + CEscore_all + genderContr * 
                  guiseContr * SAscore_all * EQscore_all, 
                  data=ind_au_df)
    
    out_pval_3 <- data.frame(summary(ind_au_lm)$coefficients[,4]) %>% 
      rownames_to_column("predictor") %>% 
      rename(pvalue = 2) %>%
      mutate(xint_method = xint_type,
             slope_quantile = i,
             model = 'gen_int') %>%
      relocate(c(xint_method, slope_quantile, model))
    
    ind_au_lm <- lm(guiseShift ~ scale(Age) + genderContr * CEscore_all * 
                  guiseContr * SAscore_all * EQscore_all, 
                  data=ind_au_df)
    
    out_pval_4 <- data.frame(summary(ind_au_lm)$coefficients[,4]) %>% 
      rownames_to_column("predictor") %>% 
      rename(pvalue = 2) %>%
      mutate(xint_method = xint_type,
             slope_quantile = i,
             model = 'all_int') %>%
      relocate(c(xint_method, slope_quantile, model))
    
    out_df <- rbind(out_df, out_pval_1, out_pval_2, out_pval_3, out_pval_4) %>% na.omit()
  }
  return(out_df)
}
```

#### Collect p-Values
```{r}
pvals_models <- model_multiverse_pvals(xints_coef_au, 'coef', 0.3) %>%
  rbind(model_multiverse_pvals(xints_fixef_au, 'fixef', 0.3)) %>%
  mutate(significant = case_when(pvalue < 0.05 ~ 1,
                                 pvalue < 0.1 ~ 0.5,
                                 TRUE ~ 0))
pvals_models
```


#### Plot Histogram of p-values

Based on these plots, showing the p-values from each combination of options together without differentiating by option, the two predictors that stand out as having particularly many instances of p-values below 0.05 are clearly: `SAscore:EQscore` and `SAscore` (but notably **not** `guise:SAscore`). 

```{r}
pvals_models %>% filter(predictor == '(Intercept)' | predictor == 'EQscore_all' | 
                          predictor == 'scale(Age)' | predictor == 'genderContr' | 
                          predictor == 'CEscore_all' | predictor == 'guiseContr' | 
                          predictor == 'guiseContr:EQscore_all' | predictor == 'guiseContr:SAscore_all' | 
                          predictor == 'SAscore_all:EQscore_all' | predictor == 'SAscore_all') %>%
  ggplot() +
  geom_histogram(aes(x=pvalue), col='blue', fill='lightblue', bins=50) +
  facet_wrap(~predictor) +
  geom_vline(xintercept=0.05, col='red', linetype='dashed') +
  theme_bw()
```

#### Plot Grid of Options

##### Model Comparison plots
It's impossible to read the values in this plot (see separate plots by model below), but from the grey shading in this composite plot, it's obvious what the patterns are. 

1. While effects of guise:SA are present in the smaller models (within a certain range of slope_quantile values), this effect disappears when more variability is accounted for. *My interpretation(?): The guise effect was not a difference actually conditioned by guise, but possibly by other variability relating to gender and CE. This is because once Gender and CE were allowed to vary within guise along with SA score, this guise effect disappears.*

2. While effects of SA and SA:EQ are present in all models, as the models get larger, the effects become fully consistent and/or much more significant above a certain slope_quantile cut-off (0.02 for Coef and 0.06 for Fixef). This seems to indcate a clear "boundary" for outliers. *My interpretation(?): That the effects show up with a lower cut-off for the Coef model compared to the Fixef seems to match with the notion that this dataset has less noise.* 

*My interpretation(?): Based on what I remember from Stats class, adding more predictors (e.g., control factors or covariates) means the model is 'soaking up' more variability, which if related to your factors of interest, can help with identifying effects that are really there but may have been partially masked by other intertwined factors (i.e., reduce confounding and/or increase predictive ability). Maybe that is what is happening in the larger models such that CE and Gender are related factors, so allowing them to vary in interactions with other factors helps to 'soak up' noise/variability. Whether this is what's happening and whether a larger model would make sense in the current design, however, I'm not sure about.*

```{r}
pvals_models %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all' |
                   predictor == 'EQscore_all') %>%
  ggplot(aes(x=xint_method, y=slope_quantile)) +
  scale_y_continuous(breaks=seq(0, 0.3, 0.05)) +
  facet_grid(model~predictor) +
  geom_tile(aes(fill=significant), linetype='solid', col='black') +
  geom_text(aes(label = round(pvalue, 3)), col='black', size=1) +
  scale_fill_gradient(low = "white", high = "grey") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

##### By-Model plots
```{r}
pvals_models %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all') %>%
  filter(model == 'key_int') %>%
  ggplot(aes(x=xint_method, y=slope_quantile)) +
  scale_y_continuous(breaks=seq(0, 0.3, 0.05)) +
  facet_grid(~predictor) +
  geom_tile(aes(fill=significant), linetype='solid', col='black') +
  geom_text(aes(label = round(pvalue, 3)), col='black') +
  scale_fill_gradient(low = "white", high = "grey") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

pvals_models %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all') %>%
  filter(model == 'ce_int') %>%
  ggplot(aes(x=xint_method, y=slope_quantile)) +
  scale_y_continuous(breaks=seq(0, 0.3, 0.05)) +
  facet_grid(~predictor) +
  geom_tile(aes(fill=significant), linetype='solid', col='black') +
  geom_text(aes(label = round(pvalue, 3)), col='black') +
  scale_fill_gradient(low = "white", high = "grey") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

pvals_models %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all') %>%
  filter(model == 'gen_int') %>%
  ggplot(aes(x=xint_method, y=slope_quantile)) +
  scale_y_continuous(breaks=seq(0, 0.3, 0.05)) +
  facet_grid(~predictor) +
  geom_tile(aes(fill=significant), linetype='solid', col='black') +
  geom_text(aes(label = round(pvalue, 3)), col='black') +
  scale_fill_gradient(low = "white", high = "grey") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

pvals_models %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all') %>%
  filter(model == 'all_int') %>%
  ggplot(aes(x=xint_method, y=slope_quantile)) +
  scale_y_continuous(breaks=seq(0, 0.3, 0.05)) +
  facet_grid(~predictor) +
  geom_tile(aes(fill=significant), linetype='solid', col='black') +
  geom_text(aes(label = round(pvalue, 3)), col='black') +
  scale_fill_gradient(low = "white", high = "grey") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

#### Summary Stats

If the mean of p-values are taken across all datsets, the values are again large enough that they don't indicate reliable significance. 

If we take the median p-value, this suggests that SA:EQ and SA may be considered robustly significant across research decisions and models, aligning with the insight provided by the histogram above.

```{r}
pvals_models %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all' |
                   predictor == 'EQscore_all') %>%
  group_by(predictor) %>%
  summarize(mean = mean(pvalue), median = median(pvalue), min=min(pvalue), max=max(pvalue))

pvals_models %>% filter(predictor == 'SAscore_all:EQscore_all' | 
                   predictor == 'guiseContr:SAscore_all' |
                   predictor == 'SAscore_all' |
                   predictor == 'EQscore_all') %>%
  filter(xint_method=='coef') %>%
  group_by(predictor) %>%
  summarize(mean = mean(pvalue), median = median(pvalue), min=min(pvalue), max=max(pvalue))
```

## Conclusion

The SA:EQ interaction appears robust across different datsets and models, though more so in larger models. It can also be sensitive to outlier exclusion at various quantile values in smaller models. In general, this multiverse analysis supports the notion that the SA:EQ is a "real" effect in the data, occuring in the majority of cases irrespective of arbitrary data analysis choices.

There appears to be consistent outlier effects (leading to null results for all predictor) when not excluding participants with slope values below the 0.2-0.6 quantile. This aligns with previous examination of the data which show that the few extremely large values of x-intercept all come from individuals with low slope values, and the reasonable interpretation that "cross-over" points from individuals who do not actually distinguish raised and unraised variants is meaningless (as there is no single point at which they perceive half of the stimuli as raised). 

The SA main effect and Guise:SA interaction are both less robust and less clear in interpretation. SA appears more robust in larger models while Guise:SA is only apparent in smaller models within a range of slope quantile cut-off values. I would be inclined to not take the Guise:SA as real, but I'm not sure how to interpret the SA effect that increase as the model included more interactions. The SA main effect does look quite robust in the larger models, and it's behavior was similar to the SA:EQ predictor.
