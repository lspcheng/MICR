#arrange(desc(starttime)) %>%
#mutate(subjNum = as.factor(rep(1:nSubj, each=(nrow(.)/nSubj)))) %>%
mutate(vowel = case_when(between(sentNum, 11, 20) ~ "AU",
between(sentNum, 21, 30) ~ "AI")) %>%
mutate(guiseCombination = case_when(conditionId == "condA" | conditionId == "condB" ~ "match",
conditionId == "condC" | conditionId == "condD" ~ "mismatch",
conditionId == "condE" | conditionId == "condF" ~ "baseline")) %>%
mutate(speakerOrder = case_when(conditionId == "condA" | conditionId == "condC" | conditionId == "condE" ~ "S3-S9",
conditionId == "condB" | conditionId == "condD" | conditionId == "condF" ~ "S9-S3")) %>%
mutate(rt=as.numeric(rt)) %>%
mutate_if(is.character, as.factor) %>%
mutate(time_elapsed_sec = time_elapsed/1000, rt_sec = rt/1000) %>%
select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, everything())
# Check data
condata
#summary(condata)
# Check number of data points per subject
# Correct number of data points is 174/180/186 = (168 + 6/12/18)
nData.bysubj <- condata %>%
group_by(conditionId, participantId, starttime) %>%
count()
nData.bysubj
# View(nData.bysubj)
# Troubleshoot duplicated participants
## Calculate number of subjects vs data files
nrow(nData.bysubj) # number of data files
(nSubj = length(unique(condata.read$participantId))) # unique subj nums
## Identify dups
nData.bysubj$dups = duplicated(nData.bysubj$participantId)
nData.bysubj %>% filter(dups==TRUE)
# Troubleshoot half data
nData.bysubj %>% filter(n<174)
# Troubleshoot duplicated participants
## Calculate number of subjects vs data files
paste("Total number of screened perception data files:",nrow(nData.bysubj)) # number of data files
paste("Total number of unique participant numbers:",length(unique(condata.read$participantId))) # unique subj nums
## Identify dups
nData.bysubj$dups = duplicated(nData.bysubj$participantId)
nData.bysubj %>% filter(dups==TRUE)
# Troubleshoot half data
nData.bysubj %>% filter(n<174)
# Check number of data points per condition (goal: 40 per condition)
(nSubj.bycond <- nData.bysubj %>%
group_by(conditionId) %>%
count())
# Check number of data points per subject
# Correct number of data points is 174/180/186 = (168 + 6/12/18)
(nData.bysubj <- condata %>%
group_by(conditionId, participantId, starttime) %>%
count())
# View(nData.bysubj)
# Check number of data points per condition (goal: 40 per condition)
(nSubj.bycond <- nData.bysubj %>%
group_by(conditionId) %>%
count())
# Check number of data points per condition (goal: 40 per condition)
(nSubj.bycond <- nData.bysubj %>%
group_by(conditionId) %>%
count())
# Troubleshoot duplicated participants
## Calculate number of subjects vs data files
paste("Total number of screened perception data files:",nrow(nData.bysubj)) # number of data files
paste("Total number of unique participant numbers:",length(unique(condata.read$participantId))) # unique subj nums
# select "Tonetest" data
# Check which participants got 4/6 or below on the headphone/attention check
condata.tones <- condata %>%
subset(trial_role == "tonetest") %>%
mutate(correct_response=tolower(correct_response)) %>%
group_by(participantId, conditionId) %>%
slice_max(order_by = time_elapsed, n = 6) # get last 6 tone trials, by largest time_elapsed
condata.tones
condata.tones <- condata.tones %>%
group_by(participantId, conditionId) %>%
summarise(tonesCorrect = sum(correct_response=="true")) %>%
ungroup()
condata.tones
# Troubleshoot half data
condata.tones %>% filter(tonesCorrect<5)
# select "Tonetest" data
# Check which participants got 4/6 or below on the headphone/attention check
condata.tones <- condata %>%
subset(trial_role == "tonetest") %>%
mutate(correct_response=tolower(correct_response)) %>%
group_by(participantId, conditionId) %>%
slice_max(order_by = time_elapsed, n = 6) # get last 6 tone trials, by largest time_elapsed
condata.tones
condata.tones <- condata.tones %>%
group_by(participantId, conditionId) %>%
summarise(tonesCorrect = sum(correct_response=="true")) %>%
ungroup()
condata.tones
# Wrangling
library(tidyverse)
library(mgsub)
# Statistics
library(brms)
# Plotting
library(ggplot2)
library(ghibli)
# Optional settings
options(dplyr.summarise.inform=F)
# select "Tonetest" data
# Check which participants got 4/6 or below on the headphone/attention check
condata.tones <- condata %>%
subset(trial_role == "tonetest") %>%
mutate(correct_response=tolower(correct_response)) %>%
group_by(participantId, conditionId) %>%
slice_max(order_by = time_elapsed, n = 6) # get last 6 tone trials, by largest time_elapsed
condata.tones
condata.tones <- condata.tones %>%
group_by(participantId, conditionId) %>%
summarise(tonesCorrect = sum(correct_response=="true")) %>%
ungroup()
condata.tones
# select "Tonetest" data
# Check which participants got 4/6 or below on the headphone/attention check
condata.tones <- condata %>%
subset(trial_role == "tonetest") %>%
mutate(correct_response=tolower(correct_response)) %>%
group_by(participantId, conditionId) %>%
slice_max(order_by = time_elapsed, n = 6) # get last 6 tone trials, by largest time_elapsed
condata.tones <- condata.tones %>%
group_by(participantId, conditionId) %>%
summarise(tonesCorrect = sum(correct_response=="true")) %>%
ungroup()
condata.tones
# Troubleshoot half data
condata.tones %>% filter(tonesCorrect<5)
# Select "Test" data + remove columns for Tonetest data
# Merge tonesCorrect column in for later decisions (e.g. if remove people with low score in tone test)
condata.test <- condata %>%
subset(trial_role == "test") %>%
subset(.,,-c(button_pressed, correct_answer, correct_response)) %>%
droplevels() %>%
merge(., condata.tones, all.x=TRUE) %>%
merge(., stim.durations.final, all.x=TRUE) %>%
select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, raised_response, everything())
# Check data
condata.test
summary(condata.test)
# Original data points
datapoints.og <- nrow(condata.test)
datapoints.og #40320
# Select "Test" data + remove columns for Tonetest data
# Merge tonesCorrect column in for later decisions (e.g. if remove people with low score in tone test)
condata.test <- condata %>%
subset(trial_role == "test") %>%
subset(.,,-c(button_pressed, correct_answer, correct_response)) %>%
droplevels() %>%
merge(., condata.tones, all.x=TRUE) %>%
merge(., stim.durations.final, all.x=TRUE) %>%
select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, raised_response, everything())
# Check data for obvious issues
summary(condata.test)
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
group_by(participantId, conditionId) %>%
summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
ungroup()
condata.test.bysubj
#DT::datatable(condata.test.bysubj,
#              options=list(scrollX = TRUE))
# Summary of total experiment times
condata.test.overall <- condata.test.bysubj %>%
summarize(median_time = median(time_elapsed_min), mean_time= mean(time_elapsed_min), min_time = min(time_elapsed_min), max_time = max(time_elapsed_min))
condata.test.overall
datapoints.og
# Check data
condata.test
summary(condata.test)
# Original data points
datapoints.og <- nrow(condata.test)
datapoints.og
# Select "Test" data + remove columns for Tonetest data
# Merge tonesCorrect column in for later decisions (e.g. if remove people with low score in tone test)
condata.test <- condata %>%
subset(trial_role == "test") %>%
subset(.,,-c(button_pressed, correct_answer, correct_response)) %>%
droplevels() %>%
merge(., condata.tones, all.x=TRUE) %>%
merge(., stim.durations.final, all.x=TRUE) %>%
select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, raised_response, everything())
condata.test
# Check data
summary(condata.test)
# Original data points
datapoints.og <- nrow(condata.test)
datapoints.og
# Original data points
(datapoints.og <- nrow(condata.test))
# Check data for obvious issues
summary(condata.test)
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
group_by(participantId, conditionId) %>%
summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
ungroup()
condata.test.bysubj
#DT::datatable(condata.test.bysubj,
#              options=list(scrollX = TRUE))
# Summary of total experiment times
condata.test.overall <- condata.test.bysubj %>%
summarize(median_time = median(time_elapsed_min), mean_time= mean(time_elapsed_min), min_time = min(time_elapsed_min), max_time = max(time_elapsed_min))
condata.test.overall
# Check data for obvious issues
summary(condata.test)
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
group_by(participantId, conditionId) %>%
summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
ungroup()
condata.test.bysubj
# Summary of total experiment times
condata.test.overall <- condata.test.bysubj %>%
summarize(median_time = median(time_elapsed_min), mean_time= mean(time_elapsed_min), min_time = min(time_elapsed_min), max_time = max(time_elapsed_min))
condata.test.overall
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
group_by(participantId, conditionId) %>%
summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
ungroup()
condata.test.bysubj
head(condata.test.bysubj)
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
group_by(participantId, conditionId) %>%
summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
ungroup()
head(condata.test.bysubj)
# Summary of total experiment times
condata.test.overall <- condata.test.bysubj %>%
summarize(median_time = median(time_elapsed_min), mean_time= mean(time_elapsed_min), min_time = min(time_elapsed_min), max_time = max(time_elapsed_min))
condata.test.overall
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
group_by(participantId, conditionId) %>%
summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
ungroup()
head(condata.test.bysubj)
# Summary of total experiment times
condata.test.overall <- condata.test.bysubj %>%
summarize(median_time = median(time_elapsed_min), mean_time= mean(time_elapsed_min), min_time = min(time_elapsed_min), max_time = max(time_elapsed_min))
condata.test.overall
# RT Outlier check
# Calculate response times that are at least 3 SDs away from the mean
condata.test.timesum <- condata.test %>%
#mutate(rt = rt/1000) %>%                     # if convert to seconds
summarize(meanTime = mean(rt), sdTime = sd(rt), minTime = min(rt), maxTime = max(rt), medianTime = median(rt), iqrTime = IQR(rt), meanStimTime = mean(dur_ms)) %>%
mutate(sd3 = sdTime*3, iqr3 = iqrTime*3, stimTime10 = meanStimTime+10000)
#mutate(lowerLimit = meanTime-iqr3, upperLimit = meanTime+iqr3)
condata.test.timesum
#quantile(condata.test$rt)
# Add columns of outlier criteria
condata.test.check <- condata.test %>%
mutate(rt.outlier.lower = rt < quart_dur_ms, rt.outlier.upper = rt > (dur_ms + 10000))
condata.test.check
# Check list of outliers that were removed
condata.test.outliers <- condata.test.check %>%
subset(rt.outlier.lower == TRUE | rt.outlier.upper == TRUE)
summary(condata.test.outliers)
# Summarize number of outliers attributed to each participant
condata.outliers.bysubj <- condata.test.outliers %>% #filter(conditionId=="condC") %>%
group_by(participantId, conditionId) %>%
count(sort=TRUE)
condata.outliers.bysubj
# RT Outlier check
# Calculate response times that are at least 3 SDs away from the mean
condata.test.timesum <- condata.test %>%
#mutate(rt = rt/1000) %>%                     # if convert to seconds
summarize(meanTime = mean(rt), sdTime = sd(rt), minTime = min(rt), maxTime = max(rt), medianTime = median(rt), iqrTime = IQR(rt), meanStimTime = mean(dur_ms)) %>%
mutate(sd3 = sdTime*3, iqr3 = iqrTime*3, stimTime10 = meanStimTime+10000)
#mutate(lowerLimit = meanTime-iqr3, upperLimit = meanTime+iqr3)
condata.test.timesum
#quantile(condata.test$rt)
# Add columns of outlier criteria
condata.test.check <- condata.test %>%
mutate(rt.outlier.lower = rt < quart_dur_ms, rt.outlier.upper = rt > (dur_ms + 10000))
condata.test.check
# Check list of outliers that were removed
condata.test.outliers <- condata.test.check %>%
subset(rt.outlier.lower == TRUE | rt.outlier.upper == TRUE)
summary(condata.test.outliers)
# Summarize number of outliers attributed to each participant
condata.outliers.bysubj <- condata.test.outliers %>% #filter(conditionId=="condC") %>%
group_by(participantId, conditionId) %>%
count(sort=TRUE)
condata.outliers.bysubj
View(condata.test.outliers)
# Wrangling
library(tidyverse)
library(mgsub)
# Statistics
library(brms)
# Plotting
library(ggplot2)
library(ghibli)
# Optional settings
options(dplyr.summarise.inform=F)
# List results files per subject
filelist <- list.files(path="./data/axb_1b/perception/", pattern=".csv",full.names=TRUE)
# Check Number of files
paste("Total number of perception results files in directory:",length(filelist))
paste("Expected number of questionnaire files:",length(filelist)-2-2) # -2 (repeats) -2 (returns)
# Read and Concatenate results
## (1) Just read and concat
# condata.read <- do.call(rbind, lapply(filelist, read.csv))
## (2) Read, concat and extract part of filename
condata.read <- do.call(rbind, lapply(filelist, function(x) cbind(read.csv(x),
starttime=strsplit(gsub(".csv","",x), "_")[[1]][3])))
## TODO: Update with subject numbers as necessary
condata.read <- condata.read %>%
# Fix data with undefined subject
mutate(participantId = replace_na(participantId, '5d1e2045a37a4d001a1fc2cb')) %>%
# Remove data from dropped subjects
subset(participantId != '5a68d1c1c0d83600010821e0') %>%   # did not finish -- RETURNED
subset(participantId != '5fcd0ee406bd7ab94ecc424c') %>%   # spokane + 50% too fast -- RETURNED
subset(participantId != '5e67f0321e4f0a0a657c1d08') %>%   # not michigan - florida to 22
subset(participantId != '5f9f96c7ea7f965f92f5d488') %>%   # not michigan - colorado to 18
subset(participantId != '5ffa01eb7dfb43387a868196') %>%   # not michigan - ny to 28
subset(participantId != '5fee8cf38ac18214460c59ed') %>%   # not michigan - pa to 15
subset(participantId != '5699d25625d9e9000db0c7cc') %>%   # 168 + 85 guy... did twice(?) and skipped all
subset(participantId != '601f04fb235cb14020c7a96f') %>%   # did twice, once with condA then condC, so can't use even full data
subset(participantId != '5fe0e7892738afa6a21cfd7c') %>%   # did not finish study/questionnaire -- RETURNED
# subset(participantId != '5fc511c32b9bc60bc189b893') %>%   # alien guy --- KEPT in the end
# Remove levels of dropped subjects
droplevels()
# select "Tonetest" data
# Check which participants got 4/6 or below on the headphone/attention check
condata.tones <- condata %>%
subset(trial_role == "tonetest") %>%
mutate(correct_response=tolower(correct_response)) %>%
group_by(participantId, conditionId) %>%
slice_max(order_by = time_elapsed, n = 6) # get last 6 tone trials, by largest time_elapsed
condata.tones <- condata.tones %>%
group_by(participantId, conditionId) %>%
summarise(tonesCorrect = sum(correct_response=="true")) %>%
ungroup()
condata.tones
# Troubleshoot half data
condata.tones %>% filter(tonesCorrect<5)
# Load stim.durations.final dataframe
load(file="./data/stim_durations.rData")
# Select "Test" data + remove columns for Tonetest data
# Merge tonesCorrect column in for later decisions (e.g. if remove people with low score in tone test)
condata.test <- condata %>%
subset(trial_role == "test") %>%
subset(.,,-c(button_pressed, correct_answer, correct_response)) %>%
droplevels() %>%
merge(., condata.tones, all.x=TRUE) %>%
merge(., stim.durations.final, all.x=TRUE) %>%
select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, raised_response, everything())
condata.test
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
group_by(participantId, conditionId) %>%
summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
ungroup()
head(condata.test.bysubj)
# Summary of total experiment times
# Check for especially short or long times
condata.test.overall <- condata.test.bysubj %>%
summarize(median_time = median(time_elapsed_min), mean_time= mean(time_elapsed_min), min_time = min(time_elapsed_min), max_time = max(time_elapsed_min))
condata.test.overall
condata.test.timesum
# RT Outlier check
# Calculate response times that are at least 3 SDs away from the mean
condata.test.timesum <- condata.test %>%
summarize(meanTime = mean(rt), sdTime = sd(rt), minTime = min(rt), maxTime = max(rt), medianTime = median(rt), iqrTime = IQR(rt), meanStimTime = mean(dur_ms)) %>%
mutate(sd3 = sdTime*3, iqr3 = iqrTime*3, stimTime10 = meanStimTime+10000)
condata.test.timesum
# Add columns of outlier criteria
condata.test.check <- condata.test %>%
mutate(rt.outlier.lower = rt < quart_dur_ms, rt.outlier.upper = rt > (dur_ms + 10000))
condata.test.check
# Check list of outliers that were removed
condata.test.outliers <- condata.test.check %>%
subset(rt.outlier.lower == TRUE | rt.outlier.upper == TRUE)
summary(condata.test.outliers)
# Summarize number of outliers attributed to each participant
condata.outliers.bysubj <- condata.test.outliers %>% #filter(conditionId=="condC") %>%
group_by(participantId, conditionId) %>%
count(sort=TRUE)
condata.outliers.bysubj
# Summarize number of outliers attributed to each participant
condata.outliers.bysubj <- condata.test.outliers %>% #filter(conditionId=="condC") %>%
group_by(participantId, conditionId) %>%
count(sort=TRUE)
condata.outliers.bysubj
# Check list of outliers that were removed
condata.test.outliers <- condata.test.check %>%
subset(rt.outlier.lower == TRUE | rt.outlier.upper == TRUE)
summary(condata.test.outliers)
# RT Outlier check
# Calculate response times that are at least 3 SDs away from the mean
condata.test.timesum <- condata.test %>%
summarize(meanTime = mean(rt), sdTime = sd(rt), minTime = min(rt), maxTime = max(rt), medianTime = median(rt), iqrTime = IQR(rt), meanStimTime = mean(dur_ms)) %>%
mutate(sd3 = sdTime*3, iqr3 = iqrTime*3, stimTime10 = meanStimTime+10000)
condata.test.timesum
# Add columns of outlier criteria
condata.test.check <- condata.test %>%
mutate(rt.outlier.lower = rt < quart_dur_ms, rt.outlier.upper = rt > (dur_ms + 10000))
condata.test.check
# RT Outlier check
# Calculate response times that are at least 3 SDs away from the mean
condata.test.timesum <- condata.test %>%
summarize(meanTime = mean(rt), sdTime = sd(rt), minTime = min(rt), maxTime = max(rt), medianTime = median(rt), iqrTime = IQR(rt), meanStimTime = mean(dur_ms)) %>%
mutate(sd3 = sdTime*3, iqr3 = iqrTime*3, stimTime10 = meanStimTime+10000)
condata.test.timesum
# Add columns of outlier criteria
condata.test.check <- condata.test %>%
mutate(rt.outlier.lower = rt < quart_dur_ms, rt.outlier.upper = rt > (dur_ms + 10000))
# Check list of outliers that were removed
condata.test.outliers <- condata.test.check %>%
subset(rt.outlier.lower == TRUE | rt.outlier.upper == TRUE)
summary(condata.test.outliers)
# Summarize number of outliers attributed to each participant
condata.outliers.bysubj <- condata.test.outliers %>% #filter(conditionId=="condC") %>%
group_by(participantId, conditionId) %>%
count(sort=TRUE)
condata.outliers.bysubj
# Final data points
datapoints.final <- nrow(condata.test.final)
# Remove outliers
condata.test.final <- setdiff(condata.test.check, condata.test.outliers)
condata.test.final
# Final data points
datapoints.final <- nrow(condata.test.final)
datapoints.final #39970 (out of 40320)
datapoints.og-datapoints.final #58
(datapoints.og-datapoints.final)/datapoints.og # = 0.008852259 = 0.8% of the data were removed due to responses that were too quick (less than 3/4 of the time into the audio file, before the third token would have played) or too slow (over 10 sec after the end of the audio file, an arbitrarily chosen value that should be enough time if a participant were responding as quickly as possible)
# Group data by StimType (i.e. Speaker-SpeakerGuise-Vowel-SentNum)
condata.test.final <- condata.test.final %>%
unite(token, speaker, sentNum, remove=FALSE) %>%
mutate(word = case_when(token == "S3_21" ~ "bright",
token == "S3_22" ~ "device",
token == "S3_30" ~ "twice",
token == "S9_23" ~ "goodnight",
token == "S9_25" ~ "invite",
token == "S9_29" ~ "sight",
token == "S3_18" ~ "slouch",
token == "S3_19" ~ "without",
token == "S3_20" ~ "workout",
token == "S9_11" ~ "checkout",
token == "S9_18" ~ "sprouts",
token == "S9_20" ~ "workout")) %>%
mutate(item = paste0(speaker,"_",word)) %>%
mutate(respRS = case_when(raised_response == "true" ~ 1,
raised_response == "false" ~ 0)) %>%
unite(stimType_byword, speaker, speakerGuise, vowel, word, remove=FALSE) %>%
unite(stimType_byvowel, speaker, speakerGuise, vowel, remove=FALSE) %>%
mutate(sentNum = as.factor(sentNum)) %>%
mutate(step = (step-5)) %>%
select(participantId, guiseCombination, step, vowel, speakerGuise, speaker, word, speakerOrder, respRS, stimType_byword, stimType_byvowel, everything())
condata.test.final
# Summary
summary(condata.test.final)
# write.csv(condata.test.final, 'data/axb_1b_exp_data.csv', row.names=F)
# Final data points
datapoints.final <- nrow(condata.test.final)
datapoints.final #39970 (out of 40320)
datapoints.og-datapoints.final #58
(datapoints.og-datapoints.final)/datapoints.og # = 0.008852259 = 0.8% of the data were removed due to responses that were too quick (less than 3/4 of the time into the audio file, before the third token would have played) or too slow (over 10 sec after the end of the audio file, an arbitrarily chosen value that should be enough time if a participant were responding as quickly as possible)
# Check original data points
(datapoints.og <- nrow(condata.test))
# Final data points
datapoints.final <- nrow(condata.test.final)
datapoints.final
# Calculate percentage of data removed
datapoints.og-datapoints.final
(datapoints.og-datapoints.final)/datapoints.og # = 0.008852259 = 0.8% of the data were removed due to responses that were too quick (less than 3/4 of the time into the audio file, before the third token would have played) or too slow (over 10 sec after the end of the audio file, an arbitrarily chosen value that should be enough time if a participant were responding as quickly as possible)
# Final kept data points
(datapoints.final <- nrow(condata.test.final))
# Final removed data points
datapoints.og-datapoints.final
# Calculate percentage of data removed
(datapoints.og-datapoints.final)/datapoints.og # = 0.008852259 = 0.8% of the data were removed due to responses that were too quick (less than 3/4 of the time into the audio file, before the third token would have played) or too slow (over 10 sec after the end of the audio file, an arbitrarily chosen value that should be enough time if a participant were responding as quickly as possible)
# Remove outliers
condata.test.final <- setdiff(condata.test.check, condata.test.outliers)
# Group data by StimType (i.e. Speaker-SpeakerGuise-Vowel-SentNum)
condata.test.final <- condata.test.final %>%
unite(token, speaker, sentNum, remove=FALSE) %>%
mutate(word = case_when(token == "S3_21" ~ "bright",
token == "S3_22" ~ "device",
token == "S3_30" ~ "twice",
token == "S9_23" ~ "goodnight",
token == "S9_25" ~ "invite",
token == "S9_29" ~ "sight",
token == "S3_18" ~ "slouch",
token == "S3_19" ~ "without",
token == "S3_20" ~ "workout",
token == "S9_11" ~ "checkout",
token == "S9_18" ~ "sprouts",
token == "S9_20" ~ "workout")) %>%
mutate(item = paste0(speaker,"_",word)) %>%
mutate(respRS = case_when(raised_response == "true" ~ 1,
raised_response == "false" ~ 0)) %>%
unite(stimType_byword, speaker, speakerGuise, vowel, word, remove=FALSE) %>%
unite(stimType_byvowel, speaker, speakerGuise, vowel, remove=FALSE) %>%
mutate(sentNum = as.factor(sentNum)) %>%
mutate(step = (step-5)) %>%
select(participantId, guiseCombination, step, vowel, speakerGuise, speaker, word, speakerOrder, respRS, stimType_byword, stimType_byvowel, everything())
condata.test.final
# Summary
summary(condata.test.final)
# Write to file
write.csv(condata.test.final, 'data/axb_1b_exp_data.csv', row.names=F)
# Final kept data points
(datapoints.final <- nrow(condata.test.final))
# Final removed data points
datapoints.og-datapoints.final
# Calculate percentage of data removed
(datapoints.og-datapoints.final)/datapoints.og # = 0.008852259 = 0.8% of the data were removed due to responses that were too quick (less than 3/4 of the time into the audio file, before the third token would have played) or too slow (over 10 sec after the end of the audio file, an arbitrarily chosen value that should be enough time if a participant were responding as quickly as possible)
