---
title: "MICR AXB Experiment 1b (AXB-p2) Analysis"
output: html_notebook
---

# Set up
## Packages
```{r, warning=F}
# Wrangling
library(tidyverse)
library(mgsub)
library(naniar)


# Statistics
library(brms)

# Plotting
library(ggplot2)
library(ghibli)

# Optional settings
options(dplyr.summarise.inform=F)
```

## Functions
```{r, warning=F}
# Standard error function
std.error <- function(x, na.rm = T) {
  sqrt(var(x, na.rm = na.rm)/length(x[complete.cases(x)]))
}

# ggplot theme
gg_theme <- function() {
  theme_bw() +
  theme(plot.title=element_text(size=25),
        plot.subtitle=element_text(size=15, face="italic"),
        axis.title=element_text(size=20),
        axis.text=element_text(size=15),
        strip.background =element_rect(fill="white"),
        strip.text = element_text(size=15))+
  theme(legend.title = element_text(size=15, face="bold"),
        legend.text=element_text(size=15))
}
```

# Perception Data
## Pre-Process Data
### Read & Clean Results

Read in the data from downloaded CSV files from Firebase. Also remove data from subjects who did not complete the study ("returned" on Prolific) or have been identified to be outliers, not following instructions, not fulfilling my participant requirements, etc. Outliers are dentified in a later section below (Outlier Check), while participant requirements are checked in the data from the questionnaire.

```{r}
# List results files per subject
filelist <- list.files(path="./data/axb_1b/perception/", pattern=".csv",full.names=TRUE) 

# Check Number of files
paste("Total number of perception results files in directory:",length(filelist)) 
paste("Expected number of questionnaire files:",length(filelist)-2-2) # -2 (repeats) -2 (returns) 
```

In this case, when reading in the data by participant file, the experiment start time is extracted from the structure `./data/axb_XX/perception/subjnumber_starttime.csv` where I first remove the ".csv" with gsub, split the string into three based on "_", then select the third item from the first output object (`[[1]][3]`).

```{r}
# Read and Concatenate results
## (1) Just read and concat
# condata.read <- do.call(rbind, lapply(filelist, read.csv))

## (2) Read, concat and extract part of filename
condata.read <- do.call(rbind, lapply(filelist, function(x) cbind(read.csv(x),
                                      starttime=strsplit(gsub(".csv","",x), "_")[[1]][3])))
## TODO: Update with subject numbers as necessary
condata.read <- condata.read %>% 
  
  # Fix data with undefined subject
  mutate(participantId = replace_na(participantId, '5d1e2045a37a4d001a1fc2cb')) %>%
  
  # Remove data from dropped subjects
  subset(participantId != '5a68d1c1c0d83600010821e0') %>%   # did not finish -- RETURNED
  subset(participantId != '5fcd0ee406bd7ab94ecc424c') %>%   # spokane + 50% too fast -- RETURNED
  subset(participantId != '5e67f0321e4f0a0a657c1d08') %>%   # not michigan - florida to 22
  subset(participantId != '5f9f96c7ea7f965f92f5d488') %>%   # not michigan - colorado to 18
  subset(participantId != '5ffa01eb7dfb43387a868196') %>%   # not michigan - ny to 28
  subset(participantId != '5fee8cf38ac18214460c59ed') %>%   # not michigan - pa to 15
  subset(participantId != '5699d25625d9e9000db0c7cc') %>%   # 168 + 85 guy... did twice(?) and skipped all
  subset(participantId != '601f04fb235cb14020c7a96f') %>%   # did twice, once with condA then condC, so can't use even full data
  subset(participantId != '5fe0e7892738afa6a21cfd7c') %>%   # did not finish study/questionnaire -- RETURNED
# subset(participantId != '5fc511c32b9bc60bc189b893') %>%   # alien guy --- KEPT in the end

  # Remove levels of dropped subjects
  droplevels()

```

```{r}
# Trim unnecesssary columns and rows
condata <- subset(condata.read,,-c(url, internal_node_id, view_history, stimulus, success, key_press, trial_index, 
                                    trial_type, wordStim))

# Keep only tonetest and test rows
# Create subject number column by order of participation (first sorting subjects by starttime, then adding subjnum column)
# Recover vowel data from sentNum column (== Vowel data was accidently left out of stimuli data)
# Manipulate data types
# Reorder columns
condata <- condata %>%
  subset(trial_role == "test" | trial_role == "tonetest") %>%
  
  #arrange(desc(starttime)) %>%
  #mutate(subjNum = as.factor(rep(1:nSubj, each=(nrow(.)/nSubj)))) %>%
  
  mutate(vowel = case_when(between(sentNum, 11, 20) ~ "AU",
                           between(sentNum, 21, 30) ~ "AI")) %>%
  mutate(guiseCombination = case_when(conditionId == "condA" | conditionId == "condB" ~ "match",
                                        conditionId == "condC" | conditionId == "condD" ~ "mismatch",
                                        conditionId == "condE" | conditionId == "condF" ~ "baseline")) %>%
  mutate(speakerOrder = case_when(conditionId == "condA" | conditionId == "condC" | conditionId == "condE" ~ "S3-S9",
                                  conditionId == "condB" | conditionId == "condD" | conditionId == "condF" ~ "S9-S3")) %>%
  
  mutate(rt=as.numeric(rt)) %>%
  mutate_if(is.character, as.factor) %>%
  mutate(time_elapsed_sec = time_elapsed/1000, rt_sec = rt/1000) %>%
  
  select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, everything())

# Check data
condata
#summary(condata)
```

### Check Results
#### Check Participants
```{r}
# Check number of data points per subject
# Correct number of data points is 174/180/186 = (168 + 6/12/18) 
(nData.bysubj <- condata %>%
  group_by(conditionId, participantId, starttime) %>%
  count())
```

```{r}
# Check number of data points per condition (goal: 40 per condition)
(nSubj.bycond <- nData.bysubj %>%
  group_by(conditionId) %>%
  count())
```

##### Troubleshoot Participant Issues
```{r}
# Manually scroll to check data as needed
View(nData.bysubj)
```

```{r}
# Troubleshoot duplicated participants
## Calculate number of subjects vs data files

paste("Total number of screened perception data files:",nrow(nData.bysubj)) # number of data files
paste("Total number of unique participant numbers:",length(unique(condata.read$participantId))) # unique subj nums
```


```{r}
## Identify dups
nData.bysubj$dups = duplicated(nData.bysubj$participantId)
nData.bysubj %>% filter(dups==TRUE)

# Troubleshoot half data
nData.bysubj %>% filter(n<174)

```

#### Check Tone Test
```{r}
# select "Tonetest" data
# Check which participants got 4/6 or below on the headphone/attention check
condata.tones <- condata %>%
  subset(trial_role == "tonetest") %>%
  mutate(correct_response=tolower(correct_response)) %>%
  group_by(participantId, conditionId) %>%
  slice_max(order_by = time_elapsed, n = 6) # get last 6 tone trials, by largest time_elapsed

condata.tones <- condata.tones %>%
  group_by(participantId, conditionId) %>%
  summarise(tonesCorrect = sum(correct_response=="true")) %>%
  ungroup()
condata.tones
```

```{r}
# Troubleshoot half data
condata.tones %>% filter(tonesCorrect<5)
```

#### Check AXB Trials

```{r}
# Load stim.durations.final dataframe
load(file="./data/stim_durations.rData")

# Select "Test" data + remove columns for Tonetest data
# Merge tonesCorrect column in for later decisions (e.g. if remove people with low score in tone test)

condata.test <- condata %>%
  subset(trial_role == "test") %>%
  subset(.,,-c(button_pressed, correct_answer, correct_response)) %>%
  droplevels() %>%
  
  merge(., condata.tones, all.x=TRUE) %>%
  merge(., stim.durations.final, all.x=TRUE) %>%
  
  select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, raised_response, everything())
condata.test
```

```{r}
# Check data for obvious issues
summary(condata.test)
```

```{r}
# Check original data points
(datapoints.og <- nrow(condata.test))
```

#### Check AXB Outliers
```{r}
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
  group_by(participantId, conditionId) %>%
  summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
  ungroup()
head(condata.test.bysubj)

# Summary of total experiment times
# Check for especially short or long times
condata.test.overall <- condata.test.bysubj %>%
  summarize(median_time = median(time_elapsed_min), mean_time= mean(time_elapsed_min), min_time = min(time_elapsed_min), max_time = max(time_elapsed_min))
condata.test.overall
```


```{r}
# RT Outlier check
# Calculate response times that are at least 3 SDs away from the mean
condata.test.timesum <- condata.test %>%
  summarize(meanTime = mean(rt), sdTime = sd(rt), minTime = min(rt), maxTime = max(rt), medianTime = median(rt), iqrTime = IQR(rt), meanStimTime = mean(dur_ms)) %>%
  mutate(sd3 = sdTime*3, iqr3 = iqrTime*3, stimTime10 = meanStimTime+10000)
condata.test.timesum
```


```{r}
# Add columns of outlier criteria 
condata.test.check <- condata.test %>%
  mutate(rt.outlier.lower = rt < quart_dur_ms, rt.outlier.upper = rt > (dur_ms + 10000))

# Check list of outliers that were removed
condata.test.outliers <- condata.test.check %>%
  subset(rt.outlier.lower == TRUE | rt.outlier.upper == TRUE)
summary(condata.test.outliers)

# Summarize number of outliers attributed to each participant
condata.outliers.bysubj <- condata.test.outliers %>% #filter(conditionId=="condC") %>%
    group_by(participantId, conditionId) %>%
    count(sort=TRUE)
condata.outliers.bysubj
```

```{r}
View(condata.test.outliers)
```


```{r}
# A
4/168 #0.02380952 subj 60053227125e504142df91e9

# C
# 85/168 #0.5059524 subj 5699d25625d9e9000db0c7cc **BUT ALSO IN COND E???
14/168 #0.08333333 subj 5e42f74f5b772a18434cabf7 --- all were longer; can keep if necessary
4/168 #0.02380952 subj 5a68d1c1c0d83600010821e0

# E
# 168/168 #0.5059524 subj 5699d25625d9e9000db0c7cc **BUT ALSO IN COND C???
# 85/168 #0.5059524 5fcd0ee406bd7ab94ecc424c --- the spokane 18-year-old
7/168 #0.04166667 subj 5fc511c32b9bc60bc189b893 --- all were shorter...; also the alien guy
5/168 #0.0297619 5fc769df1f4e27017a638e8e
4/168 #0.02380952 subj 5fdf9d13a6a9ed7d8efd0b69

```

Go back to top and remove outlier participants, if necessary. Then rerun everything up to this point.

### Finalize Results

```{r}
# Remove outliers
condata.test.final <- setdiff(condata.test.check, condata.test.outliers)

# Group data by StimType (i.e. Speaker-SpeakerGuise-Vowel-SentNum)
condata.test.final <- condata.test.final %>%
  unite(token, speaker, sentNum, remove=FALSE) %>%
  
  mutate(word = case_when(token == "S3_21" ~ "bright",
                   token == "S3_22" ~ "device",
                   token == "S3_30" ~ "twice",
                   token == "S9_23" ~ "goodnight",
                   token == "S9_25" ~ "invite",
                   token == "S9_29" ~ "sight",
                   token == "S3_18" ~ "slouch",
                   token == "S3_19" ~ "without",
                   token == "S3_20" ~ "workout",
                   token == "S9_11" ~ "checkout",
                   token == "S9_18" ~ "sprouts",
                   token == "S9_20" ~ "workout")) %>%
  mutate(item = paste0(speaker,"_",word)) %>%
  mutate(respRS = case_when(raised_response == "true" ~ 1,
                            raised_response == "false" ~ 0)) %>%
  
  unite(stimType_byword, speaker, speakerGuise, vowel, word, remove=FALSE) %>%
  unite(stimType_byvowel, speaker, speakerGuise, vowel, remove=FALSE) %>%
  
  mutate(sentNum = as.factor(sentNum)) %>%
  mutate(step = (step-5)) %>%
  
  select(participantId, guiseCombination, step, vowel, speakerGuise, speaker, word, speakerOrder, respRS, stimType_byword, stimType_byvowel, everything())

# Print data
condata.test.final

# Summary
summary(condata.test.final)

# Write to file
write.csv(condata.test.final, 'data/axb_1b_exp_data.csv', row.names=F)
```

```{r}
# Final kept data points
(datapoints.final <- nrow(condata.test.final))

# Final removed data points
datapoints.og-datapoints.final

# Calculate percentage of data removed
(datapoints.og-datapoints.final)/datapoints.og # = 0.008852259 = 0.8% of the data were removed due to responses that were too quick (less than 3/4 of the time into the audio file, before the third token would have played) or too slow (over 10 sec after the end of the audio file, an arbitrarily chosen value that should be enough time if a participant were responding as quickly as possible)
```


## Plot Data for Visualization

### Mean PropRS per Guise Condition
```{r}
# Get subj means per condition
subj.means <- condata.test.final %>% #filter(participantId!='5e42f74f5b772a18434cabf7') %>%
  group_by(participantId, step, vowel, speakerGuise) %>%
  summarise(mean.Prop = mean(respRS))

# Get group means and se per condition (by averaging speaker means)
condition.means <- subj.means %>%
  group_by(step, vowel, speakerGuise) %>%
  summarise(grandM.Prop = mean(mean.Prop), se = std.error(mean.Prop))

# Plot lineplot with error bars on step points
byGuise_prop_plot <- condition.means %>%
  ggplot(aes(x = step, y = grandM.Prop)) +
  geom_point(stat="identity", aes(colour = factor(speakerGuise)), cex=5) +
  geom_line(aes(colour=factor(speakerGuise), linetype=factor(speakerGuise)), lwd=1) +
  geom_errorbar(width = .25, aes(ymin = grandM.Prop-se, ymax = grandM.Prop+se, 
                                 colour = factor(speakerGuise))) +
  facet_grid(~vowel) +
  labs(y = "Proportion 'raised' response", x = "Continuum Step (UR to RS)", color="Guise",
       linetype = "Guise", title="Raising Perception: By Guise") +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,4,3)]) +
  gg_theme()
byGuise_prop_plot
```

### Mean RT per Guise Condition
```{r, warning=FALSE}
# Get subj means per condition
subj.means <- condata.test.final %>% filter(speaker=='S3') %>% #filter(participantId!='5e42f74f5b772a18434cabf7') %>%
  group_by(participantId, step, vowel, speakerGuise) %>%
  summarise(mean.rt = mean(rt))

# Get group means and se per condition (by averaging speaker means)
condition.means <- subj.means %>%
  group_by(step, vowel, speakerGuise) %>%
  summarise(grandM.rt = mean(mean.rt), se = std.error(mean.rt))

# Plot lineplot with error bars on step points
byGuise_rt_plot <- condition.means %>%
  ggplot(aes(x = step, y = grandM.rt)) +
  geom_point(stat="identity", aes(colour = factor(speakerGuise)), cex=5) +
  geom_line(aes(colour=factor(speakerGuise), linetype=factor(speakerGuise)), lwd=1) +
  geom_errorbar(width = .25, aes(ymin = grandM.rt-se, ymax = grandM.rt+se, colour = factor(speakerGuise))) +
  facet_grid(~vowel) +
  labs(y = "Response Time (ms)", x = "Continuum Step (UR to RS)", color="Guise",
       linetype = "Guise", title="Reaction Time: By Guise") +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,4,3)])+
  gg_theme()
byGuise_rt_plot
```

### Mean propRS per Word (and Guise)
```{r}
# Get subj means per condition
subj.means <- condata.test.final %>% filter(speaker=="S3") %>%
  group_by(participantId, step, vowel, speakerGuise, speaker, word) %>%
  summarise(mean.Prop = mean(respRS))

# Get group means and se per condition (by averaging speaker means)
condition.means <- subj.means %>%
  group_by(step, vowel, speakerGuise, speaker, word) %>%
  summarise(grandM.Prop = mean(mean.Prop), se = std.error(mean.Prop))

# AI
byWord_prop_plot <- condition.means %>% filter(vowel=="AI") %>%
  ggplot(aes(x = step, y = grandM.Prop)) +
  geom_point(stat="identity", aes(colour = factor(speakerGuise)), cex=5) +
  geom_line(aes(colour=factor(speakerGuise), linetype=factor(word)), lwd=1) +
  geom_errorbar(width = .25, aes(ymin = grandM.Prop-se, ymax = grandM.Prop+se, colour = factor(speakerGuise))) +
  facet_grid(speaker~word) +
  labs(y = "Proportion 'raised' response", x = "Continuum Step (UR to RS)", color="Guise",
       linetype = "Word", title="AI Raising Perception: By Word") +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,4,3)])+
  gg_theme()
byWord_prop_plot

# AU
byWord_prop_plot <- condition.means %>% filter(vowel=="AU") %>%
  ggplot(aes(x = step, y = grandM.Prop)) +
  geom_point(stat="identity", aes(colour = factor(speakerGuise)), cex=5) +
  geom_line(aes(colour=factor(speakerGuise), linetype=factor(word)), lwd=1) +
  geom_errorbar(width = .25, aes(ymin = grandM.Prop-se, ymax = grandM.Prop+se, colour = factor(speakerGuise))) +
  facet_grid(speaker~word) +
  labs(y = "Proportion 'raised' response", x = "Continuum Step (UR to RS)", color="Guise",
       linetype = "Word", title="AU Raising Perception: By Word") +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,4,3)])+
  gg_theme()
byWord_prop_plot

```

### Mean propRS Plots by Individual (adapted from CantoMergers project)
```{r, echo=F}
# MI guise
bySubj_prop_plot <- condata.test.final %>% filter(vowel=="AU") %>% filter(conditionId=="condA") %>%
  ggplot(aes(x=step, y=respRS, color=speakerGuise)) +
  geom_smooth(method="loess") +
  facet_wrap(~participantId) +
  geom_vline(xintercept = 0, alpha=0.5) +
  geom_hline(yintercept =  0.5, alpha = 0.5) +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  labs(title="AU Raising perception: By Participant", y="Proportion RS response", color="Guise", x="") +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(3)])+
  gg_theme()
bySubj_prop_plot

# CN guise
bySubj_prop_plot <- condata.test.final %>% filter(vowel=="AU") %>% filter(conditionId=="condC") %>%
  ggplot(aes(x=step, y=respRS, color=speakerGuise)) +
  geom_smooth(method="loess") +
  facet_wrap(~participantId) +
  geom_vline(xintercept = 0, alpha=0.5) +
  geom_hline(yintercept =  0.5, alpha = 0.5) +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  labs(title="AU Raising perception: By Participant", y="Proportion RS response", color="Guise", x="") +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(4)])+
  gg_theme()
bySubj_prop_plot

# BL guise
bySubj_prop_plot <- condata.test.final %>% filter(vowel=="AU") %>% filter(conditionId=="condE") %>%
  ggplot(aes(x=step, y=respRS, color=speakerGuise)) +
  geom_smooth(method="loess") +
  facet_wrap(~participantId) +
  geom_vline(xintercept = 0, alpha=0.5) +
  geom_hline(yintercept =  0.5, alpha = 0.5) +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  labs(title="AU Raising perception: By Participant", y="Proportion RS response", color="Guise", x="") +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(2)])+
  gg_theme()
bySubj_prop_plot
```

# Questionnaire Data
## Pre-Process Data
### Read & Clean Results
The Qualtrics output includes a text response file and a numerical response file. Because I want to use text for some questions (e.g. IK2, the word selection question) but numbers for other questions (e.g. familiarity scale questions), I need to work with both.

I have downloaded the files and renamed them as '_text' and '_num" files. Specifically, file names have been renamed from the original Qualtrics download name to `final_lbq_num.csv` and `final_lbq_text.csv`.
```{r}
# List results files per subject
filelist <- list.files(path="./data/axb_1b/questionnaire/", pattern=".csv",full.names=TRUE)

# Read and Concatenate results
# (1) Just read and concat
#condata.read <- do.call(rbind, lapply(filelist, read.csv))

# (2) Read, concat and extract part of filename
quesdata.read <- do.call(rbind, lapply(filelist, function(x) cbind(read.csv(x), dataFormat=strsplit(gsub(".csv","",x), "_")[[1]][4])))

# Check that newly extracted columns are correct (`num` or `text`)
# quesdata.read$dataFormat
```

Remove the unnecessarily columns and rows. Also extract the question text while we're at it (before removing the rows) so that we can refer to the questions as necessary, but they won't be in the data to be analysed.
```{r}
# Remove metadata columns (first several)
quesdata <- quesdata.read %>% 
  select(-c(StartDate, EndDate, Status, IPAddress, Progress, Duration..in.seconds., Finished, RecordedDate, ResponseId, RecipientLastName, RecipientFirstName, RecipientEmail, ExternalReference, LocationLatitude, LocationLongitude, DistributionChannel, UserLanguage))

# Question reference (if want to look back at question text)
questions <- quesdata[c(1),]

# Remove unecessary question header and test data rows
# Also remove data from dropped subjects
## TODO: Update with subject numbers as necessary

quesdata <- quesdata %>%
  # Remove irrelevant rows and removed data
  subset(subjID != "Please check that your Prolific ID is correct, then press the 'next' button to continue with the survey." & subjID !="{\"ImportId\":\"QID78_TEXT\"}") %>%
  subset(subjID != "preview1") %>%
  droplevels() %>%
  
  # Add subject and condition info
  rename(participantId = subjID) %>%
  merge(., condata.tones) %>%
  mutate(guiseCombination = case_when(conditionId == "condA" | conditionId == "condB" ~ "match",
                                        conditionId == "condC" | conditionId == "condD" ~ "mismatch",
                                        conditionId == "condE" | conditionId == "condF" ~ "baseline")) %>%
  mutate(speakerOrder = case_when(conditionId == "condA" | conditionId == "condC" | conditionId == "condE" ~ "S3-S9",
                                  conditionId == "condB" | conditionId == "condD" | conditionId == "condF" ~ "S9-S3")) %>%
  
    # Remove returned participants
  subset(participantId != '5fcd0ee406bd7ab94ecc424c') %>%   # spokane + 50% too fast -- RETURNED
  subset(participantId != '5e67f0321e4f0a0a657c1d08') %>%   # not michigan - florida to 22
  subset(participantId != '5f9f96c7ea7f965f92f5d488') %>%   # not michigan - colorado to 18
  subset(participantId != '5ffa01eb7dfb43387a868196') %>%   # not michigan - ny to 28
  subset(participantId != '5fee8cf38ac18214460c59ed') %>%   # not michigan - pennsylvania to 15
  droplevels()

```

#### Question Wording
Here's the table of the question tags, numbers and text. This interactive table allows for sorting and searching! We can use this to check the exact wording of the questionsâ€”all of them as they were shown to the participant.

```{r, eval=T, results='asis'}
# Print questions for reference
DT::datatable(questions, 
              options=list(scrollX = TRUE,
                           autoWidth = TRUE,
                           columnDefs = list(list(width = '200px', targets = "_all"))))
```

#### Demographic Responses

```{r}
# Subset to demographic question columns
quesdata.demo <- quesdata %>% filter(dataFormat == "text") %>%
  mutate_if(is.character, as.factor) %>%
  select(conditionId, participantId, guiseCombination, speakerOrder, 
         Age, Gender, Ethnicity, SpHDisorder, SpHDisorder_2_TEXT, Degree, LingExp, FirstLang, 
         Loc1_1:Loc6_5) %>%
  
  # Fix spelling errors and variation on demographic questions
  mutate_at(vars(-conditionId, -speakerOrder),tolower) %>%
  mutate_all(str_trim)

# Check data
quesdata.demo

# Summary
# summary(quesdata.demo)
```

#### Responses in Text Format
```{r lbqdata-text}
# Subset data to text format
quesdata.text <- quesdata %>% filter(dataFormat == "text") %>%
  select(participantId, conditionId, guiseCombination, speakerOrder, everything()) %>%
  select(-c(Age, Gender, Ethnicity, SpHDisorder, SpHDisorder_2_TEXT, Degree, LingExp, FirstLang, 
         Loc1_1:Loc6_5, expPurpose)) %>%
  mutate_if(is.character, as.factor)

# Check data
quesdata.text
```

#### Responses in Numerical Format
```{r lbqdata-num}
# Subset data to num format
quesdata.num <- quesdata %>% subset(dataFormat == "num") %>%
  select(participantId, conditionId, guiseCombination, speakerOrder, everything()) %>%
  select(-c(Age, Gender, Ethnicity, SpHDisorder, SpHDisorder_2_TEXT, Degree, LingExp, FirstLang, 
         Loc1_1:Loc6_5, expPurpose)) %>%
  mutate_at(vars(participantId:speakerOrder), as.factor) %>%
  mutate(SC0=as.numeric(SC0)) %>%
  mutate_if(~ all(grepl('^\\d+$', .x)), as.numeric)

# Check data
quesdata.num
```

### Check Results
#### Check Demographic Responses
```{r}
# Full table
ques.demo.sum <- quesdata.demo %>%
  select(conditionId, Age, Gender, Ethnicity, Loc1_5)
ques.demo.sum
```

##### Counts
```{r}
with(quesdata.demo, unique(Gender))
```


```{r}
with(quesdata.demo, unique(Loc1_5))
```

```{r}
with(quesdata.demo, unique(Ethnicity))
```

#### Clean Demographic Responses
```{r}
quesdata.demo <- quesdata.demo %>%
  
  mutate(Gender = mgsub(Gender, c("woman.*|female|cis female", "male|cis male", "non-binary|non binary|nonbinary"), c("f", "m", "nb"))) %>%
  
  mutate(Loc1_4 = mgsub(Loc1_4, c("michigan"), c("mi")), Loc2_4 = mgsub(Loc2_4, c("michigan"), c("mi")), Loc3_4 = mgsub(Loc3_4, c("michigan"), c("mi")), Loc4_4 = mgsub(Loc4_4, c("michigan"), c("mi")), Loc5_4 = mgsub(Loc5_4, c("michigan"), c("mi"))) %>%
  
  mutate(Ethnicity = mgsub(Ethnicity, c("caucasian|caucasion|american","african american|african-american", "middle easten"), c("white", "black", "middle-eastern"))) %>%
  mutate(Ethnicity = mgsub(Ethnicity, c("asian/white","white/hispanic"), c("multiracial","multiracial"), fixed=TRUE)) %>%
  mutate(Ethnicity = mgsub(Ethnicity, c(".*white.*", "black.*"), c("white", "black"))) %>%
  
  # Change vector classes from character class
  mutate(Age=as.numeric(Age)) %>%
  
  # Select
  select(conditionId, participantId, everything())

quesdata.demo <- quesdata.demo %>%
  
  # Fix answers from the one 'alien' participant
  mutate(Gender = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'm', Gender),
         LingExp = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'none', LingExp),
         Loc1_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '0', Loc1_1),
         Loc1_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '1', Loc1_2),
         Loc1_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Zeeland', Loc1_3),
         Loc1_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Ottawa', Loc1_4),
         Loc1_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'MI', Loc1_5),
         Loc2_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '1', Loc2_1),
         Loc2_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '2', Loc2_2),
         Loc2_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Sedona', Loc2_3),
         Loc2_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '', Loc2_4),
         Loc2_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Arizona', Loc2_5),
         Loc3_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '2', Loc3_1),
         Loc3_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '14', Loc3_2),
         Loc3_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Glenn', Loc3_3),
         Loc3_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Allegan', Loc3_4),
         Loc3_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'MI', Loc3_5),
         Loc4_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '14', Loc4_1),
         Loc4_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '18', Loc4_2),
         Loc4_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'South Haven', Loc4_3),
         Loc4_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Van Buren', Loc4_4),
         Loc5_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'MI', Loc4_5),
         Loc5_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '18', Loc5_1),
         Loc5_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '23', Loc5_2),
         Loc5_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Grand Rapids', Loc5_3),
         Loc5_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'Kent', Loc5_4),
         Loc5_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'MI', Loc5_5),
         Loc6_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '23', Loc6_1),
         Loc6_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '34', Loc6_2),
         Loc6_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'various... Grandville', Loc6_3),
         Loc6_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'various... Kent', Loc6_4),
         Loc6_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'various... MI', Loc6_5))
  
quesdata.demo
#summary(quesdata.demo)

```

##### Counts
```{r}
# Ethnicity Counts
ques.demo.ethn <- quesdata.demo %>%
  group_by(Ethnicity) %>%
  count()
ques.demo.ethn
```

```{r lbqdata-demo-cond}
# Descriptive stats by condition
ques.demo.age <- quesdata.demo %>%
  group_by(conditionId) %>%
  summarise(n=length(conditionId), minAge = min(Age), maxAge = max(Age), meanAge = mean(Age), sdAge = sd(Age)) %>%
  ungroup()
ques.demo.age

ques.demo.age <- quesdata.demo %>%
  group_by(conditionId, Gender) %>%
  summarise(n=length(conditionId), minAge = min(Age), maxAge = max(Age), meanAge = mean(Age), sdAge = sd(Age)) %>%
  ungroup()
ques.demo.age
```
##### Locations
```{r}
quesdata.loc <- quesdata.demo %>% select(participantId, starts_with("Loc"))
quesdata.loc

loc.code <- data.frame(Loc = unique(levels(quesdata.loc$Loc1_3))) %>%
  rbind(data.frame(Loc = unique(levels(quesdata.loc$Loc2_3)))) %>%
  rbind(data.frame(Loc = unique(levels(quesdata.loc$Loc3_3)))) %>%
  rbind(data.frame(Loc = unique(levels(quesdata.loc$Loc4_3)))) %>%
  rbind(data.frame(Loc = unique(levels(quesdata.loc$Loc5_3)))) %>%
  rbind(data.frame(Loc = unique(levels(quesdata.loc$Loc6_3)))) %>%
  unique()
loc.code
```

### Finalize Results