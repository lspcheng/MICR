---
title: "MICR AXB Experiment 1b (AXB-p2) Analysis"
output: html_notebook
---

# Set up
## Packages
```{r, warning=F}
# Wrangling
library(tidyverse)
library(mgsub)

# Statistics/Numerical processing
library(brms)

# PCA
library(FactoMineR) 
library(factoextra) 
library(GPArotation)
library(paran)

# Plotting
library(ggplot2)
library(ghibli)

# Optional settings
options(dplyr.summarise.inform=F) # Stop dplyr from printing summarise error (that isn't an error)
select <- dplyr::select # Ensure that select() command is the dplyr command (clashes with MASS, which is imported/required by paran)
```

## Functions
```{r, warning=F}
# Standard error function
std.error <- function(x, na.rm = T) {
  sqrt(var(x, na.rm = na.rm)/length(x[complete.cases(x)]))
}

# ggplot theme
gg_theme <- function() {
  theme_bw() +
  theme(plot.title=element_text(size=25),
        plot.subtitle=element_text(size=15, face="italic"),
        axis.title=element_text(size=20),
        axis.text=element_text(size=15),
        strip.background =element_rect(fill="white"),
        strip.text = element_text(size=15))+
  theme(legend.title = element_text(size=15, face="bold"),
        legend.text=element_text(size=15))
}
```

# ...
# Perception Data
## Pre-Process Data
### Read & Clean Results

Read in the data from downloaded CSV files from Firebase. Also remove data from subjects who did not complete the study ("returned" on Prolific) or have been identified to be outliers, not following instructions, not fulfilling my participant requirements, etc. Outliers are dentified in a later section below (Outlier Check), while participant requirements are checked in the data from the questionnaire.

```{r}
# List results files per subject
filelist <- list.files(path="./data/axb_1b/perception/", pattern=".csv",full.names=TRUE) 

# Check Number of files
paste("Total number of perception results files in directory:",length(filelist)) 
paste("Expected number of questionnaire files:",length(filelist)-2-2) # -2 (repeats) -2 (returns) 
```

In this case, when reading in the data by participant file, the experiment start time is extracted from the structure `./data/axb_XX/perception/subjnumber_starttime.csv` where I first remove the ".csv" with gsub, split the string into three based on "_", then select the third item from the first output object (`[[1]][3]`).

```{r}
# Read and Concatenate results
## (1) Just read and concat
# condata.read <- do.call(rbind, lapply(filelist, read.csv))

## (2) Read, concat and extract part of filename
condata.read <- do.call(rbind, lapply(filelist, function(x) cbind(read.csv(x),
                                      starttime=strsplit(gsub(".csv","",x), "_")[[1]][3])))
## TODO: Update with subject numbers as necessary
condata.read <- condata.read %>% 
  
  # Fix data with undefined subject
  mutate(participantId = replace_na(participantId, '5d1e2045a37a4d001a1fc2cb')) %>%
  
  # Remove data from dropped subjects
  subset(participantId != '5a68d1c1c0d83600010821e0') %>%   # did not finish -- RETURNED
  subset(participantId != '5fcd0ee406bd7ab94ecc424c') %>%   # spokane + 50% too fast -- RETURNED
  subset(participantId != '5e67f0321e4f0a0a657c1d08') %>%   # not michigan - florida to 22
  subset(participantId != '5f9f96c7ea7f965f92f5d488') %>%   # not michigan - colorado to 18
  subset(participantId != '5ffa01eb7dfb43387a868196') %>%   # not michigan - ny to 28
  subset(participantId != '5fee8cf38ac18214460c59ed') %>%   # not michigan - pa to 15
  subset(participantId != '5699d25625d9e9000db0c7cc') %>%   # 168 + 85 guy... did twice(?) and skipped all
  subset(participantId != '601f04fb235cb14020c7a96f') %>%   # did twice, once with condA then condC, so can't use even full data
  subset(participantId != '5fe0e7892738afa6a21cfd7c') %>%   # did not finish study/questionnaire -- RETURNED
# subset(participantId != '5fc511c32b9bc60bc189b893') %>%   # alien guy --- KEPT in the end

  # Remove levels of dropped subjects
  droplevels()

```

```{r}
# Trim unnecesssary columns and rows
condata <- subset(condata.read,,-c(url, internal_node_id, view_history, stimulus, success, key_press, trial_index, 
                                    trial_type, wordStim))

# Keep only tonetest and test rows
# Create subject number column by order of participation (first sorting subjects by starttime, then adding subjnum column)
# Recover vowel data from sentNum column (== Vowel data was accidently left out of stimuli data)
# Manipulate data types
# Reorder columns
condata <- condata %>%
  subset(trial_role == "test" | trial_role == "tonetest") %>%
  
  #arrange(desc(starttime)) %>%
  #mutate(subjNum = as.factor(rep(1:nSubj, each=(nrow(.)/nSubj)))) %>%
  
  mutate(vowel = case_when(between(sentNum, 11, 20) ~ "AU",
                           between(sentNum, 21, 30) ~ "AI")) %>%
  mutate(guiseCombination = case_when(conditionId == "condA" | conditionId == "condB" ~ "match",
                                        conditionId == "condC" | conditionId == "condD" ~ "mismatch",
                                        conditionId == "condE" | conditionId == "condF" ~ "baseline")) %>%
  mutate(speakerOrder = case_when(conditionId == "condA" | conditionId == "condC" | conditionId == "condE" ~ "S3-S9",
                                  conditionId == "condB" | conditionId == "condD" | conditionId == "condF" ~ "S9-S3")) %>%
  
  mutate(rt=as.numeric(rt)) %>%
  mutate_if(is.character, as.factor) %>%
  mutate(time_elapsed_sec = time_elapsed/1000, rt_sec = rt/1000) %>%
  
  select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, everything())

# Check data
condata
#summary(condata)
```

### Check Results
#### Check Participants
```{r}
# Check number of data points per subject
# Correct number of data points is 174/180/186 = (168 + 6/12/18) 
(nData.bysubj <- condata %>%
  group_by(conditionId, participantId, starttime) %>%
  count())
```

```{r}
# Check number of data points per condition (goal: 40 per condition)
(nSubj.bycond <- nData.bysubj %>%
  group_by(conditionId) %>%
  count())
```

Troubleshoot participant issues manually and/or by filtering duplicate subject files or those with too few responses.

```{r}
# Manually scroll to check data as needed
View(nData.bysubj)
```

```{r}
# Troubleshoot duplicated participants
## Calculate number of subjects vs data files

paste("Total number of screened perception data files:",nrow(nData.bysubj)) # number of data files
paste("Total number of unique participant numbers:",length(unique(condata.read$participantId))) # unique subj nums
```


```{r}
## Identify dups
nData.bysubj$dups = duplicated(nData.bysubj$participantId)
nData.bysubj %>% filter(dups==TRUE)

# Troubleshoot half data
nData.bysubj %>% filter(n<174)

```

#### Check Tone Test
```{r}
# select "Tonetest" data
# Check which participants got 4/6 or below on the headphone/attention check
condata.tones <- condata %>%
  subset(trial_role == "tonetest") %>%
  mutate(correct_response=tolower(correct_response)) %>%
  group_by(participantId, conditionId) %>%
  slice_max(order_by = time_elapsed, n = 6) # get last 6 tone trials, by largest time_elapsed

condata.tones <- condata.tones %>%
  group_by(participantId, conditionId) %>%
  summarise(tonesCorrect = sum(correct_response=="true")) %>%
  ungroup()
condata.tones
```

```{r}
# Troubleshoot half data
condata.tones %>% filter(tonesCorrect<5)
```

#### Check AXB Trials

```{r}
# Load stim.durations.final dataframe
load(file="./data/stim_durations.rData")

# Select "Test" data + remove columns for Tonetest data
# Merge tonesCorrect column in for later decisions (e.g. if remove people with low score in tone test)

condata.test <- condata %>%
  subset(trial_role == "test") %>%
  subset(.,,-c(button_pressed, correct_answer, correct_response)) %>%
  droplevels() %>%
  
  merge(., condata.tones, all.x=TRUE) %>%
  merge(., stim.durations.final, all.x=TRUE) %>%
  
  select(participantId, conditionId, trial_role, time_elapsed, time_elapsed_sec, rt, rt_sec, raised_response, everything())
condata.test
```

```{r}
# Check data for obvious issues
summary(condata.test)
```

```{r}
# Check original data points
(datapoints.og <- nrow(condata.test))
```

#### Check AXB Outliers
```{r}
# What are the descriptive stats on total experiment time and reaction times?
condata.test.bysubj <- condata.test %>%
  group_by(participantId, conditionId) %>%
  summarize(time_elapsed_min = max(time_elapsed_sec/60), mean_rt_sec = mean(rt_sec), min_rt_sec = min(rt_sec),  max_rt_sec = max(rt_sec), sd_rt_sec = sd(rt_sec)) %>%
  ungroup()
head(condata.test.bysubj)

# Summary of total experiment times
# Check for especially short or long times
condata.test.overall <- condata.test.bysubj %>%
  summarize(median_time = median(time_elapsed_min), mean_time= mean(time_elapsed_min), min_time = min(time_elapsed_min), max_time = max(time_elapsed_min))
condata.test.overall
```


```{r}
# RT Outlier check
# Calculate response times that are at least 3 SDs away from the mean
condata.test.timesum <- condata.test %>%
  summarize(meanTime = mean(rt), sdTime = sd(rt), minTime = min(rt), maxTime = max(rt), medianTime = median(rt), iqrTime = IQR(rt), meanStimTime = mean(dur_ms)) %>%
  mutate(sd3 = sdTime*3, iqr3 = iqrTime*3, stimTime10 = meanStimTime+10000)
condata.test.timesum
```


```{r}
# Add columns of outlier criteria 
condata.test.check <- condata.test %>%
  mutate(rt.outlier.lower = rt < quart_dur_ms, rt.outlier.upper = rt > (dur_ms + 10000))

# Check list of outliers that were removed
condata.test.outliers <- condata.test.check %>%
  subset(rt.outlier.lower == TRUE | rt.outlier.upper == TRUE)
summary(condata.test.outliers)

# Summarize number of outliers attributed to each participant
condata.outliers.bysubj <- condata.test.outliers %>% #filter(conditionId=="condC") %>%
    group_by(participantId, conditionId) %>%
    count(sort=TRUE)
condata.outliers.bysubj
```

```{r}
View(condata.test.outliers)
```


```{r}
# A
4/168 #0.02380952 subj 60053227125e504142df91e9

# C
# 85/168 #0.5059524 subj 5699d25625d9e9000db0c7cc **BUT ALSO IN COND E???
14/168 #0.08333333 subj 5e42f74f5b772a18434cabf7 --- all were longer; can keep if necessary
4/168 #0.02380952 subj 5a68d1c1c0d83600010821e0

# E
# 168/168 #0.5059524 subj 5699d25625d9e9000db0c7cc **BUT ALSO IN COND C???
# 85/168 #0.5059524 5fcd0ee406bd7ab94ecc424c --- the spokane 18-year-old
7/168 #0.04166667 subj 5fc511c32b9bc60bc189b893 --- all were shorter...; also the alien guy
5/168 #0.0297619 5fc769df1f4e27017a638e8e
4/168 #0.02380952 subj 5fdf9d13a6a9ed7d8efd0b69

```

Go back to top and remove outlier participants, if necessary. Then rerun everything up to this point.

### Finalize Results

```{r}
# Remove outliers
condata.test.final <- setdiff(condata.test.check, condata.test.outliers)

# Group data by StimType (i.e. Speaker-SpeakerGuise-Vowel-SentNum)
condata.test.final <- condata.test.final %>%
  unite(token, speaker, sentNum, remove=FALSE) %>%
  
  mutate(word = case_when(token == "S3_21" ~ "bright",
                   token == "S3_22" ~ "device",
                   token == "S3_30" ~ "twice",
                   token == "S9_23" ~ "goodnight",
                   token == "S9_25" ~ "invite",
                   token == "S9_29" ~ "sight",
                   token == "S3_18" ~ "slouch",
                   token == "S3_19" ~ "without",
                   token == "S3_20" ~ "workout",
                   token == "S9_11" ~ "checkout",
                   token == "S9_18" ~ "sprouts",
                   token == "S9_20" ~ "workout")) %>%
  mutate(item = paste0(speaker,"_",word)) %>%
  mutate(respRS = case_when(raised_response == "true" ~ 1,
                            raised_response == "false" ~ 0)) %>%
  
  unite(stimType_byword, speaker, speakerGuise, vowel, word, remove=FALSE) %>%
  unite(stimType_byvowel, speaker, speakerGuise, vowel, remove=FALSE) %>%
  
  mutate(sentNum = as.factor(sentNum)) %>%
  mutate(step = (step-5)) %>%
  
  select(participantId, guiseCombination, step, vowel, speakerGuise, speaker, word, speakerOrder, respRS, stimType_byword, stimType_byvowel, everything())

# Summary
summary(condata.test.final)

# Print data
condata.test.final

# Write to file
write.csv(condata.test.final, 'data/axb_1b_exp_data.csv', row.names=F)
```

```{r}
# Final kept data points
(datapoints.final <- nrow(condata.test.final))

# Final removed data points
datapoints.og-datapoints.final

# Calculate percentage of data removed
(datapoints.og-datapoints.final)/datapoints.og # = 0.008852259 = 0.8% of the data were removed due to responses that were too quick (less than 3/4 of the time into the audio file, before the third token would have played) or too slow (over 10 sec after the end of the audio file, an arbitrarily chosen value that should be enough time if a participant were responding as quickly as possible)
```


## Summarize Data
### Plots
#### PropRS by Guise
```{r}
# Get subj means per condition
subj.means <- condata.test.final %>% #filter(participantId!='5e42f74f5b772a18434cabf7') %>%
  group_by(participantId, step, vowel, speakerGuise) %>%
  summarise(mean.Prop = mean(respRS))

# Get group means and se per condition (by averaging speaker means)
condition.means <- subj.means %>%
  group_by(step, vowel, speakerGuise) %>%
  summarise(grandM.Prop = mean(mean.Prop), se = std.error(mean.Prop))

# Plot lineplot with error bars on step points
byGuise_prop_plot <- condition.means %>%
  ggplot(aes(x = step, y = grandM.Prop)) +
  geom_point(stat="identity", aes(colour = factor(speakerGuise)), cex=5) +
  geom_line(aes(colour=factor(speakerGuise), linetype=factor(speakerGuise)), lwd=1) +
  geom_errorbar(width = .25, aes(ymin = grandM.Prop-se, ymax = grandM.Prop+se, 
                                 colour = factor(speakerGuise))) +
  facet_grid(~vowel) +
  labs(y = "Proportion 'raised' response", x = "Continuum Step (UR to RS)", color="Guise",
       linetype = "Guise", title="Raising Perception: By Guise") +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,4,3)]) +
  gg_theme()
byGuise_prop_plot
```

#### RT by Guise
```{r, warning=FALSE}
# Get subj means per condition
subj.means <- condata.test.final %>% filter(speaker=='S3') %>% #filter(participantId!='5e42f74f5b772a18434cabf7') %>%
  group_by(participantId, step, vowel, speakerGuise) %>%
  summarise(mean.rt = mean(log(rt)))

# Get group means and se per condition (by averaging speaker means)
condition.means <- subj.means %>%
  group_by(step, vowel, speakerGuise) %>%
  summarise(grandM.rt = mean(mean.rt), se = std.error(mean.rt))

# Plot lineplot with error bars on step points
byGuise_rt_plot <- condition.means %>%
  ggplot(aes(x = step, y = grandM.rt)) +
  geom_point(stat="identity", aes(colour = factor(speakerGuise)), cex=5) +
  geom_line(aes(colour=factor(speakerGuise), linetype=factor(speakerGuise)), lwd=1) +
  geom_errorbar(width = .25, aes(ymin = grandM.rt-se, ymax = grandM.rt+se, colour = factor(speakerGuise))) +
  facet_grid(~vowel) +
  labs(y = "Log Response Time", x = "Continuum Step (UR to RS)", color="Guise",
       linetype = "Guise", title="Reaction Time: By Guise") +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,4,3)])+
  gg_theme()
byGuise_rt_plot
```

#### PropRS by Word
```{r}
# Get subj means per condition
subj.means <- condata.test.final %>% filter(speaker=="S3") %>%
  group_by(participantId, step, vowel, speakerGuise, speaker, word) %>%
  summarise(mean.Prop = mean(respRS))

# Get group means and se per condition (by averaging speaker means)
condition.means <- subj.means %>%
  group_by(step, vowel, speakerGuise, speaker, word) %>%
  summarise(grandM.Prop = mean(mean.Prop), se = std.error(mean.Prop))

# AI
byWord_prop_plot <- condition.means %>% filter(vowel=="AI") %>%
  ggplot(aes(x = step, y = grandM.Prop)) +
  geom_point(stat="identity", aes(colour = factor(speakerGuise)), cex=5, alpha=0.75) +
  geom_line(aes(colour=factor(speakerGuise), linetype=factor(word)), lwd=1) +
  geom_errorbar(width = .25, aes(ymin = grandM.Prop-se, ymax = grandM.Prop+se, colour = factor(speakerGuise))) +
  facet_grid(speaker~word) +
  labs(y = "Proportion 'raised' response", x = "Continuum Step (UR to RS)", color="Guise",
       linetype = "Word", title="AI Raising Perception: By Word") +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,4,3)])+
  gg_theme()
byWord_prop_plot

# AU
byWord_prop_plot <- condition.means %>% filter(vowel=="AU") %>%
  ggplot(aes(x = step, y = grandM.Prop)) +
  geom_point(stat="identity", aes(colour = factor(speakerGuise)), cex=5, alpha=0.75) +
  geom_line(aes(colour=factor(speakerGuise), linetype=factor(word)), lwd=1) +
  geom_errorbar(width = .25, aes(ymin = grandM.Prop-se, ymax = grandM.Prop+se, colour = factor(speakerGuise))) +
  facet_grid(speaker~word) +
  labs(y = "Proportion 'raised' response", x = "Continuum Step (UR to RS)", color="Guise",
       linetype = "Word", title="AU Raising Perception: By Word") +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,4,3)])+
  gg_theme()
byWord_prop_plot

```

#### PropRS by Individual
(adapted from CantoMergers project)
```{r, echo=F}
# MI guise
bySubj_prop_plot <- condata.test.final %>% filter(vowel=="AU") %>% filter(conditionId=="condA") %>%
  ggplot(aes(x=step, y=respRS, color=speakerGuise)) +
  geom_smooth(method="loess") +
  facet_wrap(~participantId) +
  geom_vline(xintercept = 0, alpha=0.5) +
  geom_hline(yintercept =  0.5, alpha = 0.5) +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  labs(title="AU Raising perception: By Participant", y="Proportion RS response", color="Guise", x="") +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(3)])+
  gg_theme()
bySubj_prop_plot

# CN guise
bySubj_prop_plot <- condata.test.final %>% filter(vowel=="AU") %>% filter(conditionId=="condC") %>%
  ggplot(aes(x=step, y=respRS, color=speakerGuise)) +
  geom_smooth(method="loess") +
  facet_wrap(~participantId) +
  geom_vline(xintercept = 0, alpha=0.5) +
  geom_hline(yintercept =  0.5, alpha = 0.5) +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  labs(title="AU Raising perception: By Participant", y="Proportion RS response", color="Guise", x="") +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(4)])+
  gg_theme()
bySubj_prop_plot

# BL guise
bySubj_prop_plot <- condata.test.final %>% filter(vowel=="AU") %>% filter(conditionId=="condE") %>%
  ggplot(aes(x=step, y=respRS, color=speakerGuise)) +
  geom_smooth(method="loess") +
  facet_wrap(~participantId) +
  geom_vline(xintercept = 0, alpha=0.5) +
  geom_hline(yintercept =  0.5, alpha = 0.5) +
  coord_cartesian(ylim=c(0, 1)) +
  scale_x_continuous(breaks = -3:3) +
  labs(title="AU Raising perception: By Participant", y="Proportion RS response", color="Guise", x="") +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(2)])+
  gg_theme()
bySubj_prop_plot
```

# ...
# Questionnaire Data
## Pre-Process Data
### Read & Clean Results
The Qualtrics output includes a text response file and a numerical response file. Because I want to use text for some questions (e.g. IK2, the word selection question) but numbers for other questions (e.g. familiarity scale questions), I need to work with both.

I have downloaded the files and renamed them as '_text' and '_num" files. Specifically, file names have been renamed from the original Qualtrics download name to `final_lbq_num.csv` and `final_lbq_text.csv`.
```{r, warning=F}
# List results files per subject
filelist <- list.files(path="./data/axb_1b/questionnaire/", pattern=".csv",full.names=TRUE)

# Read and Concatenate results
# (1) Just read and concat
#condata.read <- do.call(rbind, lapply(filelist, read.csv))

# (2) Read, concat and extract part of filename
quesdata.read <- do.call(rbind, lapply(filelist, function(x) cbind(read.csv(x), dataFormat=strsplit(gsub(".csv","",x), "_")[[1]][4])))

# Check that newly extracted columns are correct (`num` or `text`)
# quesdata.read$dataFormat

colnames(quesdata.read)

```

Remove the unnecessarily columns and rows. Also extract the question text while we're at it (before removing the rows) so that we can refer to the questions as necessary, but they won't be in the data to be analysed.
```{r}
# Remove metadata columns (first several)
quesdata <- quesdata.read %>% 
  select(-c(StartDate, EndDate, Status, IPAddress, Progress, Duration..in.seconds., Finished, RecordedDate, ResponseId, RecipientLastName, RecipientFirstName, RecipientEmail, ExternalReference, LocationLatitude, LocationLongitude, DistributionChannel, UserLanguage, PROLIFIC_PID))

# Question reference (if want to look back at question text)
questions <- quesdata %>% slice(1)

# Remove unecessary question header and test data rows + add/fix relevant info + remove data from dropped subjects
quesdata <- quesdata %>%
  # Remove irrelevant rows and removed data
  subset(subjID != "Please check that your Prolific ID is correct, then press the 'next' button to continue with the survey." & subjID !="{\"ImportId\":\"QID78_TEXT\"}") %>%
  subset(subjID != "preview1") %>%

  # Merge with perception data (for convenience, uses the minimal dataframe `condata.tones`)
  ## Adds subject and condition info + automatically drops subjects that were screened out based on perception experiment performance/returns
  rename(participantId = subjID) %>%
  merge(., condata.tones) %>%
  mutate(guiseCombination = case_when(conditionId == "condA" | conditionId == "condB" ~ "match",
                                        conditionId == "condC" | conditionId == "condD" ~ "mismatch",
                                        conditionId == "condE" | conditionId == "condF" ~ "baseline")) %>%
  mutate(speakerOrder = case_when(conditionId == "condA" | conditionId == "condC" | conditionId == "condE" ~ "S3-S9",
                                  conditionId == "condB" | conditionId == "condD" | conditionId == "condF" ~ "S9-S3")) %>%
  droplevels()

# Fix answers from the one 'alien' participant
quesdata <- quesdata %>%
  mutate(Gender = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'm', Gender),
         LingExp = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'none', LingExp),
         Loc1_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '0', Loc1_1),
         Loc1_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '1', Loc1_2),
         Loc1_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'zeeland', Loc1_3),
         Loc1_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'ottawa', Loc1_4),
         Loc1_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'mi', Loc1_5),
         Loc2_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '1', Loc2_1),
         Loc2_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '2', Loc2_2),
         Loc2_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'sedona', Loc2_3),
         Loc2_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '', Loc2_4),
         Loc2_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'arizona', Loc2_5),
         Loc3_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '2', Loc3_1),
         Loc3_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '14', Loc3_2),
         Loc3_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'glenn', Loc3_3),
         Loc3_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'allegan', Loc3_4),
         Loc3_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'mi', Loc3_5),
         Loc4_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '14', Loc4_1),
         Loc4_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '18', Loc4_2),
         Loc4_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'south haven', Loc4_3),
         Loc4_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'van buren', Loc4_4),
         Loc5_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'mi', Loc4_5),
         Loc5_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '18', Loc5_1),
         Loc5_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '23', Loc5_2),
         Loc5_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'grand rapids', Loc5_3),
         Loc5_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'kent', Loc5_4),
         Loc5_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'mi', Loc5_5),
         Loc6_1 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '23', Loc6_1),
         Loc6_2 = ifelse(participantId=='5fc511c32b9bc60bc189b893', '34', Loc6_2),
         Loc6_3 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'various... grandville', Loc6_3),
         Loc6_4 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'various... kent', Loc6_4),
         Loc6_5 = ifelse(participantId=='5fc511c32b9bc60bc189b893', 'various... mi', Loc6_5))
```

#### Question Wording
Here's the table of the question tags, numbers and text. This interactive table allows for sorting and searching! We can use this to check the exact wording of the questions—all of them as they were shown to the participant.

```{r, eval=T, results='asis'}
# Print questions for reference
DT::datatable(questions, 
              options=list(scrollX = TRUE,
                           autoWidth = TRUE,
                           columnDefs = list(list(width = '200px', targets = "_all"))))
```

### Process Demographic Info
```{r}
# Subset to demographic question columns
quesdata.demo <- quesdata %>% filter(dataFormat == "text") %>%
  select(conditionId, participantId, guiseCombination, speakerOrder, 
         Age, Gender, Ethnicity, SpHDisorder, SpHDisorder_2_TEXT, Degree, LingExp, FirstLang, 
         Loc1_1:Loc6_5) %>%
  
  # Fix spelling errors and variation on demographic questions
  mutate_at(vars(-conditionId, -speakerOrder),tolower) %>%
  mutate_if(is.character, str_trim)

# Check data
quesdata.demo

# Summary
# summary(quesdata.demo)
```

#### Check Values
```{r}
# Quick check via table
ques.demo.sum <- quesdata.demo %>%  select(conditionId, Age, Gender, Ethnicity, Loc1_5)
```

```{r}
with(quesdata.demo, unique(Gender))
```


```{r}
with(quesdata.demo, unique(Loc1_5))
```

```{r}
with(quesdata.demo, unique(Ethnicity))
```

#### Clean Free Responses
```{r}
quesdata.demo <- quesdata.demo %>%
  
  mutate(Gender = mgsub(Gender, c("woman.*|female|cis female", "male|cis male", "non-binary|non binary|nonbinary"), c("f", "m", "nb"))) %>%
  
  mutate_at(vars(starts_with("Loc") & ends_with("_5")), ~ mgsub(.x, c("michigan.*", "mi.*"), c("mi", "mi"))) %>%
  
  mutate(Ethnicity = mgsub(Ethnicity, c("caucasian|caucasion|american|mezzogiorno","african american|african-american", "middle easten"), c("white", "black", "middle-eastern"))) %>%
  mutate(Ethnicity = mgsub(Ethnicity, c("asian/white","white/hispanic"), c("multiracial","multiracial"), fixed=TRUE)) %>%
  mutate(Ethnicity = mgsub(Ethnicity, c(".*white.*", "black.*", ".*asian"), c("white", "black", "asian"))) %>%
  
  # Change vector classes from character class
  mutate_at(vars(Age), as.numeric) %>%
  
  # Select
  select(conditionId, participantId, everything())
  
quesdata.demo
```

#### Descriptive Stats
```{r}
# Ethnicity Counts
quesdata.demo %>% group_by(Ethnicity) %>% count() 

quesdata.demo %>% group_by(Ethnicity, conditionId) %>% count() %>% pivot_wider(Ethnicity, names_from = conditionId, values_from=n)

```
```{r}
# Gender Counts
quesdata.demo %>% group_by(Gender) %>% count()

quesdata.demo %>% group_by(Gender, conditionId) %>% count() %>% pivot_wider(Gender, names_from = conditionId, values_from=n)

```

```{r}
# Age
quesdata.demo %>% summarise(n=length(conditionId), minAge = min(Age), maxAge = max(Age), meanAge = mean(Age), sdAge = sd(Age))

quesdata.demo %>% group_by(conditionId) %>%
  summarise(n=length(conditionId), minAge = min(Age), maxAge = max(Age), meanAge = mean(Age), sdAge = sd(Age)) %>% ungroup()

```

#### Locations List (under dev)
```{r}
quesdata.loc <- quesdata.demo %>% select(participantId, starts_with("Loc"))
quesdata.loc

loc.code <- data.frame(Loc = unique((quesdata.loc$Loc1_3))) %>%
  rbind(data.frame(Loc = unique((quesdata.loc$Loc2_3)))) %>%
  rbind(data.frame(Loc = unique((quesdata.loc$Loc3_3)))) %>%
  rbind(data.frame(Loc = unique((quesdata.loc$Loc4_3)))) %>%
  rbind(data.frame(Loc = unique((quesdata.loc$Loc5_3)))) %>%
  rbind(data.frame(Loc = unique((quesdata.loc$Loc6_3)))) %>%
  unique()
loc.code
```


### Process Text Responses
```{r lbqdata-text}
# Subset data to text format
quesdata.text <- quesdata %>% filter(dataFormat == "text") %>%
  select(participantId, conditionId, guiseCombination, speakerOrder, everything()) %>%
  select(-c(Age, Gender, Ethnicity, SpHDisorder, SpHDisorder_2_TEXT, Degree, LingExp, FirstLang, 
         Loc1_1:Loc6_5, expPurpose)) %>%
  mutate_if(is.character, as.factor)

quesdata.text.sub <- quesdata.text %>% 
  select(-starts_with("Q"), -SC0, -tonesCorrect) %>%
  droplevels()
quesdata.text.sub
```

#### IK2: Word Selection Question
IK2 (Implicit Knowledge Q2) refers to the question where subjects select all the words they think Canadians and Michiganders would pronounce differently ("Which of the following words, if any, do you think would be pronounced differently by someone from Canada, as opposed to someone from Michigan? Please select all that apply."). The question includes 30 words, 5 of which are target /au/ words and 5 of which are target /ai/ words.

We want to go from the raw data, the selected words, to the number of target words (/au/ and /ai/ raising words) selected. The response format is words separated by commas in one cell. So, we need to separate the strings into separate words, check whether each target word occured, then tabulate the scores. A simple search for word strings won't work, because some words (e.g. _out_) are substrings of other words (e.g. _about_). 

My solution was to first separate the single column by commas into multiple columns, one per word. To check whether the word was a response, I implement a count of 1 in a new column if a search function finds the target string in a row. This is done for each target word. Finally, I sum the count columns for /ai/ and /au/ separately to get a score out of 5.

```{r, warning=FALSE}
# IK2: Can Word Selection question

# Select only IK2 question + create copied column of Words (for next step)
ik2 <- select(quesdata.text.sub, participantId, IK2.CanWords)
ik2 <- mutate(ik2, Words = IK2.CanWords)

# Separate Words into columns (by comma) + create new columns of word in list per row
# Number of columns must be the same for every row, so find the max number of words selected by any subject (23 here)
# Each subject will have 23 columns, one word per column (Word_1, Word_2...) until no more words (NA if no word)
ik2 <- separate(ik2, "Words", paste("Word", 1:30, sep="_"), sep=",", extra="drop")

# Identify target words in each subjects' responses
# Count 1 if word exists in row; 0 if none
# /au/ targets
ik2 <- mutate(ik2, IK2.au.out = as.integer(apply(ik2, 1, function(x) any(x %in% "out"))),
              IK2.au.about = as.integer(grepl('about',IK2.CanWords)),
              IK2.au.devout = as.integer(grepl('devout',IK2.CanWords)),
              IK2.au.house = as.integer(grepl('house',IK2.CanWords)),
              IK2.au.pouch = as.integer(grepl('pouch',IK2.CanWords)))

# /ai/ targets
ik2 <- mutate(ik2, IK2.ai.like = as.integer(apply(ik2, 1, function(x) any(x %in% "like"))),
              IK2.ai.right = as.integer(apply(ik2, 1, function(x) any(x %in% "right"))),
              IK2.ai.might = as.integer(apply(ik2, 1, function(x) any(x %in% "might"))),
              IK2.ai.unite = as.integer(grepl('unite',IK2.CanWords)),
              IK2.ai.ripe = as.integer(apply(ik2, 1, function(x) any(x %in% "ripe"))))

# Sum of targets selected for /au/ and /ai/
ik2 <- mutate(ik2, IK2.au = rowSums(select(ik2, IK2.au.out:IK2.au.pouch)),
              IK2.ai = rowSums(select(ik2, IK2.ai.like:IK2.ai.ripe)))

# Here are two versions of the data
# ...with score for each target word + sum of /au/ and /ai/ targets selected
ik2.values <- select(ik2, participantId, IK2.au, IK2.ai, IK2.au.out:IK2.ai.ripe)

# ...with only sum of /au/ and /ai/ targets selected
ik2.sum <- select(ik2, participantId, IK2.au:IK2.ai)

```



### Process Num Responses
A few cases where participants do not respond to a question because the answer is 'no' results in an 'NA' entry. These 'NA's for specific columns are adjusted "manually" to the correct value. 

```{r lbqdata-num}
# Subset data to num format
quesdata.num <- quesdata %>% subset(dataFormat == "num") %>%
  select(participantId, conditionId, guiseCombination, speakerOrder, everything()) %>%
  select(-c(Age, Gender, Ethnicity, SpHDisorder, SpHDisorder_2_TEXT, Degree, LingExp, FirstLang, 
         Loc1_1:Loc6_5, expPurpose)) %>%
  rename(EQ.raws = SC0) %>%
  mutate_at(vars(participantId:speakerOrder), as.factor) %>%
  mutate_if(~ all(grepl('^\\d+$', .x)), as.numeric)

## Further adjustments
quesdata.num.sub <- quesdata.num %>%
  # Convert columns to numeric, leaving non-numeric columns as NA
  mutate_if(is.character, as.numeric) %>%
  # Remove columns that are all NA (specifically, if Sum of that column's NAs is NOT equal to the number of rows, select )
  select_if(colSums(is.na(.)) != nrow(.)) %>%
  # Remove columns of certain types of questions or specific question number
  select(-starts_with("Q"), -starts_with("Lang"), -starts_with("IK2"), -starts_with("ME1"), -ends_with("TEXT")) %>%

  # Adjust values of NA (for cases where they can be interpreted as a 'no' or 'never')
  mutate(Travel.NE_Visits = coalesce(Travel.NE_Visits, 1), Travel.S_Visits = coalesce(Travel.S_Visits, 1),
         Travel.W_Visits = coalesce(Travel.W_Visits, 1), Travel.Can_Visits = coalesce(Travel.Can_Visits, 1),
         Travel.NE_Time = coalesce(Travel.NE_Time, 1), Travel.S_Time = coalesce(Travel.S_Time, 1),
         Travel.W_Time = coalesce(Travel.W_Time, 1), Travel.Can_Time = coalesce(Travel.Can_Time, 1),
         PE1.Relatives = coalesce(PE1.Relatives, 2), PE1.CloseFamFriends = coalesce(PE1.CloseFamFriends, 2),
         EK3.CanAI.Diff = coalesce(EK3.CanAI.Diff, 2), EK4.CanAU.Diff = coalesce(EK4.CanAU.Diff, 2),
         ME3.Sources.OtherMedia_1 = coalesce(ME3.Sources.OtherMedia_1, 1), 
         ME3.Sources.Other_1 = coalesce(ME3.Sources.Other_1, 1),
         ME3.Sources.News_1 = coalesce(ME3.Sources.News_1, 1)) %>%
  droplevels()

quesdata.num.sub
```

### Finalize Cleaned Results

```{r, warning=F}
# Select and Merge only relavant numerical columns of data for PCA analysis
quesdata.clean <- quesdata.demo %>%
  select(participantId, conditionId, guiseCombination, speakerOrder, Age, Gender, Ethnicity) %>%
  merge(., ik2.values) %>%
  merge(., quesdata.num.sub) %>%
  droplevels()

# quesdata.final
quesdata.clean
```

## Analyze Data

### PCA: Awareness & Experience

```{r}
# Check correlations, which motivate the use of PCA to reduce dimensionality
with(quesdata.clean, cor.test(IK2.ai, IK2.au))

# Test correlations of similar questions
with(quesdata.clean, cor.test(Travel.Can_Visits, Travel.Can_Time))
with(quesdata.clean, cor.test(SE1.Fam.oot_1, SE2.Freq.Overall_1))
with(quesdata.clean, cor.test(SE2.Freq.Overall_1, SE2.Freq.Recent_1))
with(quesdata.clean, cor.test(SE2.Freq.Overall_1, SE2.Freq.Child_1))
with(quesdata.clean, cor.test(SE2.Freq.Recent_1, SE2.Freq.Child_1))
```

#### Run PCA

Packages required for this PCA:
* `PCA` command from `FactoMineR` library (see index for more info)
* `paran` command from `paran` library
* `Varimax` command from `GPArotations` library (https://stats.stackexchange.com/questions/59213/how-to-compute-varimax-rotated-principal-components-in-r)

Packages for visualization of PCA
* `fviz_pca_ind` and `fviz_pca_biplot` from `factoextra` 

```{r lbqdata-pca2-prep}
# (0) Select data for PCA — only numerical columns
# names(quesdata.clean)

# Medium trimmed set — removes general Canadian stereotype experience/knowledge, imitation, Sources of CE, hockey
quesdata.pca.med <- select(quesdata.clean, 
                           IK2.au, IK2.ai, 
                           SE1.Fam.oot_1, SE2.Freq.Recent_1:SE2.Freq.Overall_1, 
                           PE2.CanSpeakFreq.Recent_1:PE2.CanSpeakFreq.Overall_1,
                           ME4.CanHearFreq.Recent_1:ME4.CanHearFreq.Overall_1)
quesdata.pca.med

```

```{r}
## (1) Run Parallel Analysis with `paran`
# Standard way to decide on the number of factors or components needed in an FA or PCA.
# Prints out a scree plot as well, with the randomized line + unadjusted line
paran(quesdata.pca.med,
      graph = TRUE, color = TRUE, 
      col = c("black", "red", "blue"), lty = c(1, 2, 3), lwd = 1, legend = TRUE, 
      file = "", width = 640, height = 640, grdevice = "png", seed = 0)
```

```{r, warning=F}
## (2) Run PCA with `FactoMineR`
# ncp = number of components; adjust after checking the parallel analysis output

# FactoMineR PCA Commands
#plbqPCA        # lists commands
#plbqPCA$var    # variables
#plbqPCA$ind    # individuals
#plbqPCA$call   # summary stats

lbqPCA <- PCA(quesdata.pca.med, scale.unit = T, ncp =2, graph=F)

## Relevant Raw PCA Output
# Eigenvalues & percent variance accounted for
(eigenvalues <- lbqPCA$eig)

# Eigenvectors (=Factor matrix, factor score coefficients; sometimes called the factor, but NOT factor scores)
(eigenvectors <- lbqPCA$var$coord)

# Factor loadings (eigenvectors scaled by the square root of their associated eigenvalues)
# Calculate factor loadings using the output eigenvectors and eigenvalues
rawLoadings <- sweep(lbqPCA$var$coord,2,sqrt(lbqPCA$eig[1:ncol(lbqPCA$var$coord),1]),FUN="/")
rawLoadings

# Factor scores for each subject and dimension (also: Individual coordinate scores; principle coordinates)
rawScores <- lbqPCA$ind$coord

```

```{r}
## (3) Conduct rotation on the PCA factor loadings with `GPArotation`
# Rotations are typically done on the retained component factor loadings, not on all components nor on the eigenvectors
# Performed for ease of interpretation, maximizing factor loadings
(rotLoadings <- Varimax(rawLoadings)$loadings)

# Recover Rotation matrix from loadings
# Because the rotLoadings are calculated from rawLoadings %*% rotMatrix, can recover rotMatrix by rotLoadings "divided" by rawLoadings, which in matrix multiplication is multiplying by the inverse (transpose) 
# Note: For some reason, can't call Varimax(rawLoadings)$rotmat (just get NULL); this recreates the same matrix from Varimax(rawLoadings)
(rotMatrixL <- t(rawLoadings) %*% rotLoadings)

# Calculate rotated factor scores
# The formula simply multiplies the normalized variable scores with the rotation matrix to get rotated factor scores
# First, z-score the raw scores using base R scale()
# Then, matrix multiply the matrix of zScores with the rotation matrix
# Result is a matrix with columns=components and rows=each subject
zScores <- scale(rawScores)
rotScores <- zScores %*% rotMatrixL
```

```{r}
# Check individual rotated factor scores
rotScores
```

#### Plot PCA
```{r}
## (4) Data Visualization of Raw Scores with `factoextra`

# Plot individual factor scores
fviz_pca_ind(lbqPCA, col.ind = "#00AFBB", repel = TRUE)

# Biplot, including individual scores and factor vectors
fviz_pca_biplot(lbqPCA, label = "all", col.ind = "#00AFBB", col.var="black", ggtheme = theme_minimal())
```

```{r}
## (5) Manual Plots of Rotated Scores with `ggplot`

## Create dataframes of the rotated factor loading and factor score matrices
# Convert rotated factor loadings matrix to data frame; add variable number
rotLoadingsData <- as.data.frame(rotLoadings)
rotLoadingsData <- mutate(rotLoadingsData, variable = row.names(rotLoadings))
rotLoadingsData <- mutate(rotLoadingsData, variable = factor(variable))

# Convert rotated factor score matrix to data frame; add subject number
rotScoreData <- as.data.frame(rotScores)
rotScoreData <- rotScoreData %>% mutate(subject = 1:nrow(.))

## Create base plots
# Loading plot
loadingplot <- rotLoadingsData %>% ggplot(aes(x=Dim.1, y=Dim.2))+
  geom_segment(data=rotLoadingsData, mapping=aes(x=0, y=0, xend=Dim.1*4, yend=Dim.2*4), arrow=arrow(), size=0.5, color="black") +
  geom_text(data=rotLoadingsData, aes(x=Dim.1*4, y=Dim.2*4, label=variable), color="red",check_overlap=T) +
  scale_x_continuous(lim=c(-2.75, 2.75),breaks=seq(-3,3,1)) +
  scale_y_continuous(lim=c(-3.5, 3.5),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Variables - PCA", x="Dim 1 (36.4%)", y="Dim 2 (17.7%)") +
  gg_theme()
loadingplot

# Scatter plot of Individual factor scores
dimplot = ggplot(rotScoreData, aes(x=Dim.1, y=Dim.2))+
  geom_point(na.rm=TRUE, color="#00AFBB") +
  geom_text(aes(label=subject),hjust=1.5,vjust=1.5, color="#00AFBB", check_overlap=T)+
  scale_x_continuous(lim=c(-2.75, 2.75),breaks=seq(-3,3,1)) +
  scale_y_continuous(lim=c(-3.5, 3.5),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Individuals - PCA", x="Dim 1 (36.4%)", y="Dim 2 (17.7%)") +
  gg_theme()
dimplot

## Merge loading and score plot = Biplot

# Biplot of factor loadings + ind factor scores
ggplot(rotScoreData, aes(x=Dim.1, y=Dim.2))+
  geom_point(na.rm=TRUE, color="#00AFBB") +
  geom_text(aes(label=subject),hjust=1.5,vjust=1.5, color="#00AFBB", check_overlap=T)+
  
  # Overlay loading plot (i.e. arrows)
  geom_segment(data=rotLoadingsData, mapping=aes(x=0, y=0, xend=Dim.1*4, yend=Dim.2*4), arrow=arrow(), size=0.5, color="black") +
  geom_text(data=rotLoadingsData, aes(x=Dim.1*4.5, y=Dim.2*4.5, label=variable), color="red",check_overlap=T, nudge_y = 0)+

  scale_x_continuous(lim=c(-2.75, 2.75),breaks=seq(-3,3,1)) +
  scale_y_continuous(lim=c(-3.5, 3.5),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Biplot - PCA", x="Dim 1 (36.4%)", y="Dim 2 (17.7%)") +
  gg_theme()

```

#### Interpret PCA
```{r}
## Interpret PCs (Dimensions) based on factor loadings
rotLoadings.df <- as.data.frame(rotLoadings) %>%
  rownames_to_column(., "Variables") %>%
  rename(., "PC1"= "Dim.1", "PC2" = "Dim.2")
rotLoadings.df
```

```{r}
# PC1 Only Contributors
rotLoadings.df %>% filter(abs(PC1) > 0.2)
```


```{r}
# PC2 Only Contributors
rotLoadings.df %>% filter(abs(PC2) > 0.2)
```

```{r, warning=F}
# Check for overlapping contributors
rotLoadings.df %>% filter(abs(PC2) > 0.2 & abs(PC1) > 0.2)

# Check for non-contributors
rotLoadings.df %>% filter(abs(PC2) < 0.2 & abs(PC1) < 0.2)
```



### Finalize Results w/ PCA

```{r}
# For Merging: Convert rotated factor score matrix to data frame; add participantId (assuming order of input dataframe)
indPCAdata <- rotScoreData %>%
  mutate(participantId = quesdata.clean$participantId) %>%
  rename(CEscore = Dim.1) %>%
  rename(SAscore = Dim.2)

# Merge participant data with PC scores 
# Only select the main relevant scores
quesdata.final <- quesdata.clean %>%
  merge(., indPCAdata) %>%
  mutate(MSscore = scale(SK2.MichvStand)) %>%
  mutate(EQscore = scale(EQ.raws)) %>%
  mutate_at(vars(Age), as.numeric) %>%
  mutate_if(is.character, as.factor) %>%
  select(participantId, conditionId, guiseCombination, speakerOrder, Age, Gender, Ethnicity, CEscore, SAscore, MSscore, EQscore, EQ.raws, everything())

summary(quesdata.final)

quesdata.final

# Write to file
write.csv(quesdata.final, 'data/axb_1b_lbq_data.csv', row.names=F)

```

## Summarize Data
### Tables
#### All Scores
```{r}
# Total
quesdata.final %>% select(participantId, conditionId, guiseCombination, Age, Gender, Ethnicity, CEscore, SAscore, MSscore, EQscore, EQ.raws) %>%
  summarise_if(is.numeric, list(mean = mean, sd = sd, min = min, max = max)) %>% 
  pivot_longer(everything(), names_to = c("Measure", "Stat"), names_sep = "_", values_to = "value") %>%
  pivot_wider(Measure, names_from = "Stat", values_from = "value") %>%
  mutate(mean = round(mean, digits = 6)) %>%
  mutate(group = "Total") %>%
  select(group, everything())

?summarise_if
```


```{r}
# Baseline
quesdata.final %>% select(participantId, conditionId, guiseCombination, Age, Gender, Ethnicity, CEscore, SAscore, MSscore, EQscore, EQ.raws) %>%
  filter(guiseCombination == "baseline") %>%
  summarise_if(is.numeric, list(mean = mean, sd = sd, min = min, max = max)) %>% 
  pivot_longer(everything(), names_to = c("Measure", "Stat"), names_sep = "_", values_to = "value") %>%
  pivot_wider(Measure, names_from = "Stat", values_from = "value") %>%
  mutate(mean = round(mean, digits = 6)) %>%
  mutate(group = "baseline") %>%
  select(group, everything())

# Match
quesdata.final %>% select(participantId, conditionId, guiseCombination, Age, Gender, Ethnicity, CEscore, SAscore, MSscore, EQscore, EQ.raws) %>%
  filter(guiseCombination == "match") %>%
  summarise_if(is.numeric, list(mean = mean, sd = sd, min = min, max = max)) %>% 
  pivot_longer(everything(), names_to = c("Measure", "Stat"), names_sep = "_", values_to = "value") %>%
  pivot_wider(Measure, names_from = "Stat", values_from = "value") %>%
  mutate(mean = round(mean, digits = 6)) %>%
  mutate(group = "match") %>%
  select(group, everything())

# Mismatch
quesdata.final %>% select(participantId, conditionId, guiseCombination, Age, Gender, Ethnicity, CEscore, SAscore, MSscore, EQscore, EQ.raws) %>%
  filter(guiseCombination == "mismatch") %>%
  summarise_if(is.numeric, list(mean = mean, sd = sd, min = min, max = max)) %>% 
  pivot_longer(everything(), names_to = c("Measure", "Stat"), names_sep = "_", values_to = "value") %>%
  pivot_wider(Measure, names_from = "Stat", values_from = "value") %>%
  mutate(mean = round(mean, digits = 6)) %>%
  mutate(group = "mismatch") %>%
  select(group, everything())
```

#### PCA Contributors
```{r}
#quesdata.final.sum <- 
  quesdata.final %>% group_by(guiseCombination) %>%
  summarize(#meanEQ = mean(EQscore), 
            meanIK2.au = mean(IK2.au), meanIK2.ai = mean(IK2.ai), 
            #meanDecideCan = mean(IK1.DecideCanada), 
            #meanEhFam = mean(SE1.Fam.eh_1), 
            meanOotFam = mean(SE1.Fam.oot_1), 
            meanSEFreq = mean(SE2.Freq.Overall_1), meanSEFreq.C = mean(SE2.Freq.Child_1), 
            meanSEFreq.R = mean(SE2.Freq.Recent_1), 
            meanSEAcc = mean(SE3.Accuracy))

#quesdata.final.sum <- 
  quesdata.final %>% group_by(guiseCombination) %>%
  summarize(CanHearFreq = mean(ME4.CanHearFreq.Overall_1), 
            CanHearFreq.R = mean(ME4.CanHearFreq.Recent_1),
            CanHearFreq.C = mean(ME4.CanHearFreq.Child_1),
            CanSpeakFreq = mean(PE2.CanSpeakFreq.Overall_1),
            CanSpeakFreq.R = mean(PE2.CanSpeakFreq.Recent_1),
            CanSpeakFreq.C = mean(PE2.CanSpeakFreq.Child_1))
```

#### Yes/No Responses
```{r}
quesdata.final %>% group_by(guiseCombination) %>% mutate(EK1.CanSpeak = ifelse(EK1.CanSpeak==1, "yes", "no")) %>%
count(EK1.CanSpeak) %>%
  pivot_wider(names_from = EK1.CanSpeak, values_from = n) %>%
  mutate(question="SpeakDiff") %>% relocate(question)

quesdata.final %>% group_by(guiseCombination) %>% mutate(EK2.CanPron = ifelse(EK2.CanPron==1, "yes", "no")) %>%
count(EK2.CanPron) %>%
  pivot_wider(names_from = EK2.CanPron, values_from = n)  %>%
  mutate(question="PronounceDiff") %>% relocate(question)

quesdata.final %>% group_by(guiseCombination) %>% mutate(EK3.CanAI = ifelse(EK3.CanAI==1, "yes", "no")) %>%
count(EK3.CanAI) %>%
  pivot_wider(names_from = EK3.CanAI, values_from = n) %>%
  mutate(prop.yes = yes/(yes+no))  %>%
  mutate(question="aiDiff") %>% relocate(question)

quesdata.final %>% group_by(guiseCombination) %>% mutate(EK4.CanAU = ifelse(EK4.CanAU==1, "yes", "no")) %>%
count(EK4.CanAU) %>%
  pivot_wider(names_from = EK4.CanAU, values_from = n)  %>%
  mutate(prop.yes = yes/(yes+no))  %>%
  mutate(question="auDiff") %>% relocate(question)
```


```{r}
# Overall total
quesdata.final %>% mutate(EK3.CanAI = ifelse(EK3.CanAI==1, "yes", "no")) %>% count(EK3.CanAI) %>%
  pivot_wider(names_from = EK3.CanAI, values_from = n) %>% mutate(prop.yes = yes/(yes+no))  %>%
  mutate(question="aiDiff") %>% relocate(question) %>%
  rbind(.,quesdata.final %>% mutate(EK4.CanAU = ifelse(EK4.CanAU==1, "yes", "no")) %>% count(EK4.CanAU) %>%
          pivot_wider(names_from = EK4.CanAU, values_from = n)%>% mutate(prop.yes = yes/(yes+no))  %>%
          mutate(question="auDiff") %>% relocate(question)
  )

```

### Plots
#### EQ Scores
```{r}
# Density Plot of score distributions
quesdata.final %>% mutate(guiseCombination = plyr::mapvalues(guiseCombination, from = c("baseline", "match", "mismatch"), to = c("No-Guise", "Guise-Match", "Guise-Mismatch"))) %>%
  ggplot(aes(x=EQscore,fill=guiseCombination,color=guiseCombination))+
   geom_density(alpha=0.3) +
  labs(title="", x="EQ Score", y="Count") +
  gg_theme()

```
```{r}
# By Gender
ggplot(data=quesdata.final, aes(x=Gender, y=EQscore)) +
  geom_boxplot(aes(linetype = Gender), fill="grey", alpha=0.3, na.rm=TRUE) +
  gg_theme()

# By Gender + Guise
ggplot(data=quesdata.final, aes(x=Gender, y=EQscore)) +
  geom_boxplot(aes(fill = guiseCombination, color=guiseCombination, linetype=Gender), alpha=0.3, na.rm=TRUE) +
  facet_grid(~guiseCombination) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  scale_fill_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  gg_theme()

# By Guise
ggplot(data=quesdata.final, aes(x=guiseCombination, y=EQscore)) +
  geom_boxplot(aes(fill = guiseCombination, color=guiseCombination), alpha=0.3, na.rm=TRUE) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  scale_fill_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  gg_theme()

```

#### SA Scores

```{r}
# Density Plot of score distributions
quesdata.final %>% mutate(guiseCombination = plyr::mapvalues(guiseCombination, from = c("baseline", "match", "mismatch"), to = c("No-Guise", "Guise-Match", "Guise-Mismatch"))) %>%
  ggplot(aes(x=SAscore,fill=guiseCombination,color=guiseCombination))+
   geom_density(alpha=0.3) +
  labs(title="", x="SA Score", y="Count") +
  gg_theme()

```

```{r}
# By Gender
ggplot(data=quesdata.final, aes(x=Gender, y=SAscore)) +
  geom_boxplot(aes(linetype = Gender), fill="grey", alpha=0.3, na.rm=TRUE) +
  gg_theme()

# By Gender + Guise
ggplot(data=quesdata.final, aes(x=Gender, y=SAscore)) +
  geom_boxplot(aes(fill = guiseCombination, color=guiseCombination, linetype=Gender), alpha=0.3, na.rm=TRUE) +
  facet_grid(~guiseCombination) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  scale_fill_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  gg_theme()

# By Guise
ggplot(data=quesdata.final, aes(x=guiseCombination, y=SAscore)) +
  geom_boxplot(aes(fill = guiseCombination, color=guiseCombination), alpha=0.3, na.rm=TRUE) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  scale_fill_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  gg_theme()

```

#### CE Scores

```{r}
# Density Plot of score distributions
quesdata.final %>% mutate(guiseCombination = plyr::mapvalues(guiseCombination, from = c("baseline", "match", "mismatch"), to = c("No-Guise", "Guise-Match", "Guise-Mismatch"))) %>%
  ggplot(aes(x=CEscore,fill=guiseCombination,color=guiseCombination))+
   geom_density(alpha=0.3) +
  labs(title="", x="CE Score", y="Count") +
  gg_theme()

```

```{r}
# By Gender
ggplot(data=quesdata.final, aes(x=Gender, y=CEscore)) +
  geom_boxplot(aes(linetype = Gender), fill="grey", alpha=0.3, na.rm=TRUE) +
  gg_theme()

# By Gender + Guise
ggplot(data=quesdata.final, aes(x=Gender, y=CEscore)) +
  geom_boxplot(aes(fill = guiseCombination, color=guiseCombination, linetype=Gender), alpha=0.3, na.rm=TRUE) +
  facet_grid(~guiseCombination) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  scale_fill_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  gg_theme()

# By Guise
ggplot(data=quesdata.final, aes(x=guiseCombination, y=CEscore)) +
  geom_boxplot(aes(fill = guiseCombination, color=guiseCombination), alpha=0.3, na.rm=TRUE) +
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  scale_fill_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  gg_theme()

```

#### PCA Output Scores
```{r}

CSCE.biplot <- quesdata.final %>% mutate(guiseCombination = plyr::mapvalues(guiseCombination, from = c("baseline", "match", "mismatch"), to = c("No-Guise", "Guise-Match", "Guise-Mismatch"))) %>%
ggplot(aes(y=CEscore, x=SAscore, color=guiseCombination, shape=guiseCombination)) + 
  geom_point(na.rm=TRUE, size=3, alpha=0.7) +
  #geom_text(aes(label=subject),hjust=1.5,vjust=1.5, color="#00AFBB", check_overlap=T)+

  scale_x_continuous(lim=c(-2.5, 2.5),breaks=seq(-3,3,1)) +
  scale_y_continuous(lim=c(-3.5, 3.5),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(y="CE Scores (PC1)", x="SA Scores (PC2)", color="Participant Group", shape="Participant Group") +
  
  scale_color_manual(values=ghibli_palette("PonyoMedium")[c(6,3,4)]) +
  gg_theme()
CSCE.biplot

#ggsave(path="plots", filename="CS-CE_distribution_plot.png", CSCE.biplot, width=16, height=8, units = "in" , dpi=72)

```

#### Correlations

```{r}
quesdata.final %>% 
  ggplot(aes(x = SAscore, y = EQscore)) + 
  geom_point(stat="identity", aes(colour = factor(Gender)), cex=2) +
  geom_smooth(method="lm") +

  labs(y = "EQ Score", x = "SA Score", color="Gender",
       title="Correlations: By Speaker") +
  scale_color_manual(values=ghibli_palette("PonyoLight")[c(4,3,2)])+
  gg_theme()
```

```{r}
quesdata.final %>% 
  ggplot(aes(x = SAscore, y = CEscore)) + 
  geom_point(stat="identity", aes(colour = factor(Gender)), cex=2) +
  geom_smooth(method="lm") +

  labs(y = "CE Score", x = "SA Score", color="Gender",
       title="Correlations: By Speaker") +
  scale_color_manual(values=ghibli_palette("PonyoLight")[c(4,3,2)])+
  gg_theme()
```

```{r}
quesdata.final %>% 
  ggplot(aes(x = CEscore, y = EQscore)) + 
  geom_point(stat="identity", aes(colour = factor(Gender)), cex=2) +
  geom_smooth(method="lm") +

  labs(y = "EQ Score", x = "CE Score", color="Gender",
       title="Correlations: By Speaker") +
  scale_color_manual(values=ghibli_palette("PonyoLight")[c(4,3,2)])+
  gg_theme()
```